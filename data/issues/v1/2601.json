{
  "url": "https://api.github.com/repos/shap/shap/issues/2601",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2601/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2601/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2601/events",
  "html_url": "https://github.com/shap/shap/issues/2601",
  "id": 1279680304,
  "node_id": "I_kwDOBHDcK85MRl8w",
  "number": 2601,
  "title": "DeepExplainer doesn't work with global_max_pooling1d",
  "user": {
    "login": "tmclouisluk",
    "id": 18186155,
    "node_id": "MDQ6VXNlcjE4MTg2MTU1",
    "avatar_url": "https://avatars.githubusercontent.com/u/18186155?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/tmclouisluk",
    "html_url": "https://github.com/tmclouisluk",
    "followers_url": "https://api.github.com/users/tmclouisluk/followers",
    "following_url": "https://api.github.com/users/tmclouisluk/following{/other_user}",
    "gists_url": "https://api.github.com/users/tmclouisluk/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/tmclouisluk/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/tmclouisluk/subscriptions",
    "organizations_url": "https://api.github.com/users/tmclouisluk/orgs",
    "repos_url": "https://api.github.com/users/tmclouisluk/repos",
    "events_url": "https://api.github.com/users/tmclouisluk/events{/privacy}",
    "received_events_url": "https://api.github.com/users/tmclouisluk/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2022-06-22T07:21:15Z",
  "updated_at": "2022-11-09T10:15:09Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "My model is the following structure\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 400, 512)]        0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 400, 128)          295424    \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 400, 64)           41216     \r\n_________________________________________________________________\r\nglobal_max_pooling1d (Global (None, 64)                0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                4160      \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 65        \r\n=================================================================\r\nTotal params: 340,865\r\nTrainable params: 340,865\r\nNon-trainable params: 0\r\n```\r\n\r\nI want to use DeepExplainer into that \r\n\r\n```\r\ne = shap.DeepExplainer(model, train)\r\nshap_values = e.shap_values(test)\r\n```\r\n\r\nbut it pops this error\r\n\r\n```\r\n`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/tmp/ipykernel_18417/831445522.py in <module>\r\n----> 1 shap_values = e.shap_values(test)\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/__init__.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    122             were chosen as \"top\".\r\n    123         \"\"\"\r\n--> 124         return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    310                 # run attribution computation graph\r\n    311                 feature_ind = model_output_ranks[j,i]\r\n--> 312                 sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)\r\n    313 \r\n    314                 # assign the attributions to the right part of the output arrays\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in run(self, out, model_inputs, X)\r\n    370 \r\n    371                 return final_out\r\n--> 372             return self.execute_with_overridden_gradients(anon)\r\n    373 \r\n    374     def custom_grad(self, op, *grads):\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in execute_with_overridden_gradients(self, f)\r\n    406         # define the computation graph for the attribution values using a custom gradient-like computation\r\n    407         try:\r\n--> 408             out = f()\r\n    409         finally:\r\n    410             # reinstate the backpropagatable check\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in anon()\r\n    363                     v = tf.constant(data, dtype=self.model_inputs[i].dtype)\r\n    364                     inputs.append(v)\r\n--> 365                 final_out = out(inputs)\r\n    366                 try:\r\n    367                     tf_execute.record_gradient = tf_backprop._record_gradient\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    883 \r\n    884       with OptionalXlaContext(self._jit_compile):\r\n--> 885         result = self._call(*args, **kwds)\r\n    886 \r\n    887       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    931       # This is the first call of __call__, so we have to initialize.\r\n    932       initializers = []\r\n--> 933       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    934     finally:\r\n    935       # At this point we know that the initialization is complete (or less\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    758     self._concrete_stateful_fn = (\r\n    759         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 760             *args, **kwds))\r\n    761 \r\n    762     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   3064       args, kwargs = None, None\r\n   3065     with self._lock:\r\n-> 3066       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   3067     return graph_function\r\n   3068 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3461 \r\n   3462           self._function_cache.missed.add(call_context_key)\r\n-> 3463           graph_function = self._create_graph_function(args, kwargs)\r\n   3464           self._function_cache.primary[cache_key] = graph_function\r\n   3465 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3306             arg_names=arg_names,\r\n   3307             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3308             capture_by_value=self._capture_by_value),\r\n   3309         self._function_attributes,\r\n   3310         function_spec=self.function_spec,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\r\n   1005         _, original_func = tf_decorator.unwrap(python_func)\r\n   1006 \r\n-> 1007       func_outputs = python_func(*func_args, **func_kwargs)\r\n   1008 \r\n   1009       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    666         # the function a weak reference to itself to avoid a reference cycle.\r\n    667         with OptionalXlaContext(compile_with_xla):\r\n--> 668           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    669         return out\r\n    670 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    992           except Exception as e:  # pylint:disable=broad-except\r\n    993             if hasattr(e, \"ag_error_metadata\"):\r\n--> 994               raise e.ag_error_metadata.to_exception(e)\r\n    995             else:\r\n    996               raise\r\n\r\nValueError: in user code:\r\n\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:252 grad_graph  *\r\n        x_grad = tape.gradient(out, shap_rAnD)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:1090 gradient  **\r\n        unconnected_gradients=unconnected_gradients)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:159 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:378 custom_grad\r\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:588 handler\r\n        return nonlinearity_1d_handler(input_ind, explainer, op, *grads)\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:615 nonlinearity_1d_handler\r\n        grads[0] * tf.tile((xout - rout) / delta_in0, dup0)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\r\n        raise e\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\r\n        return func(x, y, name=name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1543 truediv\r\n        return _truediv_python3(x, y, name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1482 _truediv_python3\r\n        return gen_math_ops.real_div(x, y, name=name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py:7515 real_div\r\n        \"RealDiv\", x=x, y=y, name=name)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal\r\n        compute_device)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal\r\n        op_def=op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2042 __init__\r\n        control_input_ops, op_def)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Dimensions must be equal, but are 4 and 400 for '{{node gradient_tape/model/global_max_pooling1d/truediv_1}} = RealDiv[T=DT_FLOAT](gradient_tape/model/global_max_pooling1d/sub_1, gradient_tape/model/global_max_pooling1d/sub)' with input shapes: [4,64], [4,400,64].\r\n```\r\n\r\nI tried to add this line in front of DeepExplainer but it doesn't work as well\r\n`shap.explainers._deep.deep_tf.op_handlers[\"MaxPool\"] = shap.explainers._deep.deep_tf.passthrough`\r\n\r\nIs there any solution? Thanks",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2601/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2601/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
