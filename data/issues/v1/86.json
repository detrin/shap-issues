{
  "url": "https://api.github.com/repos/shap/shap/issues/86",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/86/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/86/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/86/events",
  "html_url": "https://github.com/shap/shap/issues/86",
  "id": 323044007,
  "node_id": "MDU6SXNzdWUzMjMwNDQwMDc=",
  "number": 86,
  "title": "Question about SHAP value intuition",
  "user": {
    "login": "albertmishaan",
    "id": 25085193,
    "node_id": "MDQ6VXNlcjI1MDg1MTkz",
    "avatar_url": "https://avatars.githubusercontent.com/u/25085193?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/albertmishaan",
    "html_url": "https://github.com/albertmishaan",
    "followers_url": "https://api.github.com/users/albertmishaan/followers",
    "following_url": "https://api.github.com/users/albertmishaan/following{/other_user}",
    "gists_url": "https://api.github.com/users/albertmishaan/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/albertmishaan/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/albertmishaan/subscriptions",
    "organizations_url": "https://api.github.com/users/albertmishaan/orgs",
    "repos_url": "https://api.github.com/users/albertmishaan/repos",
    "events_url": "https://api.github.com/users/albertmishaan/events{/privacy}",
    "received_events_url": "https://api.github.com/users/albertmishaan/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 19,
  "created_at": "2018-05-15T02:24:05Z",
  "updated_at": "2018-10-02T14:30:35Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Just had a question on SHAP values from the perspective of human intuition. I recently ran a KNN method on a non-linear dataset (a \"XOR\" dataset that resembles a 2x2 chessboard). There are four types of points, and two classifications:\r\n\r\n1. (x > 0, y > 0) Output = 0\r\n2. (x < 0, y < 0) Output = 0\r\n3. (x > 0, y < 0) Output = 1\r\n4. (x < 0, y > 0) Output = 1\r\n\r\nThe decision boundary would be the axes of the graph (ie x = 0 and y = 0). Given this, I would expect an explanation of the classification of any point to be the values of the x- and y-values themselves. For instance for a point (-3.4, 2.1), the reason why it should be classified as a \"1\" is because the x-value is 3.4 units to the left of the line x = 0 and the y-value is 2.1 units above the line y = 0. However, the SHAP value assigns all the value of the explanation to the x-value (ie the SHAP value of the y-value is 0). Why might this be? How can an explanation assign no effect to the y-value - after all, if it were negative, then the Output would be \"0\"?\r\n\r\nAlthough this question is a little academic, I would appreciate any insight you might have into this. Thanks!",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/86/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/86/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
