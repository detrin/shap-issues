{
  "url": "https://api.github.com/repos/shap/shap/issues/1707",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1707/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1707/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1707/events",
  "html_url": "https://github.com/shap/shap/issues/1707",
  "id": 777127269,
  "node_id": "MDU6SXNzdWU3NzcxMjcyNjk=",
  "number": 1707,
  "title": "Why does LinearExplainer summary_plot with plot_type=\"bar\" sometimes produce different feature importance order than model.coef_?",
  "user": {
    "login": "insuquot",
    "id": 65504486,
    "node_id": "MDQ6VXNlcjY1NTA0NDg2",
    "avatar_url": "https://avatars.githubusercontent.com/u/65504486?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/insuquot",
    "html_url": "https://github.com/insuquot",
    "followers_url": "https://api.github.com/users/insuquot/followers",
    "following_url": "https://api.github.com/users/insuquot/following{/other_user}",
    "gists_url": "https://api.github.com/users/insuquot/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/insuquot/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/insuquot/subscriptions",
    "organizations_url": "https://api.github.com/users/insuquot/orgs",
    "repos_url": "https://api.github.com/users/insuquot/repos",
    "events_url": "https://api.github.com/users/insuquot/events{/privacy}",
    "received_events_url": "https://api.github.com/users/insuquot/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2020-12-31T20:07:41Z",
  "updated_at": "2020-12-31T20:07:41Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I just started learning about Shapley values. Before finding out about shap, I got feature importance of a logistic regression model by sorting the coefficients by absolute value descending.\r\n\r\nIf I use the same logistic regression model with LinearExplainer's summary_plot (plot_type=\"bar\"), the order of the feature importances is a little different. The order changes a bit if I set feature_perturbation=\"correlation_dependent\". \r\n\r\nCan you explain the differences between these feature importance sorting methods?\r\n1. without shap, coefficients sorted by absolute value\r\n2. shap.LinearExplainer with feature_perturbation=\"interventional\"\r\n3. shap.LinearExplainer with feature_perturbation=\"correlation_dependent\"\r\n\r\nMy dependence plots with feature_perturbation=\"correlation_dependent\" look more like the plots in all the shap examples. If I set feature_perturbation to the default \"interventional\", each feature value gets assigned the same shap value for all observations. Why is correlation_dependent not the default for LinearExplainer?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1707/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1707/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
