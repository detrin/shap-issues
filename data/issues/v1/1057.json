{
  "url": "https://api.github.com/repos/shap/shap/issues/1057",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1057/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1057/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1057/events",
  "html_url": "https://github.com/shap/shap/issues/1057",
  "id": 568489898,
  "node_id": "MDU6SXNzdWU1Njg0ODk4OTg=",
  "number": 1057,
  "title": "PyTorch DeepExplainer, RNNs, and Element-Wise Multiplication",
  "user": {
    "login": "u500",
    "id": 61288919,
    "node_id": "MDQ6VXNlcjYxMjg4OTE5",
    "avatar_url": "https://avatars.githubusercontent.com/u/61288919?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/u500",
    "html_url": "https://github.com/u500",
    "followers_url": "https://api.github.com/users/u500/followers",
    "following_url": "https://api.github.com/users/u500/following{/other_user}",
    "gists_url": "https://api.github.com/users/u500/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/u500/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/u500/subscriptions",
    "organizations_url": "https://api.github.com/users/u500/orgs",
    "repos_url": "https://api.github.com/users/u500/repos",
    "events_url": "https://api.github.com/users/u500/events{/privacy}",
    "received_events_url": "https://api.github.com/users/u500/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 5,
  "created_at": "2020-02-20T18:30:56Z",
  "updated_at": "2020-09-22T02:51:33Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Two issues, they could be related:\r\n\r\n1. If I use an RNN-based module in my model, the following error comes up when trying to compute shap values: \"RuntimeError: cudnn RNN backward can only be called in training mode\".\r\n\r\n2. Assuming I replace the RNN above with a CNN, if I perform element-wise multiplication (for example, a simple attention mechanism for classification), the resulting shap values fail their additivity check (diffs > 1e-2, sometimes much larger).\r\n\r\nSimple example:\r\n\r\nx: (batchsize, timesteps, in_dim)\r\n\r\n```conv = nn.Conv1d(in_dim, out_dim, kernel_size=1)    # kernel_size of 1 just for illustration\r\nW = nn.Linear(out_dim, 1)\r\noutput = nn.Linear(out_dim, 1)\r\n\r\nh = conv(x.transpose(-1, -2)).transpose(-1, -2)    #  (batchsize, timesteps, out_dim)\r\na = W(h)    #  (batchsize, timesteps, 1)\r\na = F.softmax(a, dim=1)    #  Issue still arises when excluding this line\r\n```\r\n\r\nThis fails (shap values incorrect, diffs > 1e-2):\r\n\r\n```\r\nc = (h * a).sum(dim=1)\r\ny = output(c)\r\n```\r\n\r\nThis works, but is not helpful (shap sum correct, diffs < 1e-7):\r\n\r\n```\r\nc = (h + a).sum(dim=1)\r\ny = output(c)\r\n```\r\n\r\nI'm just wondering if I'm missing something basic since this seems pretty straightforward. I don't think a more involved full working example is required since it seems like I might not be understanding something fundamental about how DeepExplainer is operating, but I would be happy to provide one if necessary.\r\n\r\nThank you.\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1057/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1057/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
