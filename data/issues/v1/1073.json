{
  "url": "https://api.github.com/repos/shap/shap/issues/1073",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1073/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1073/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1073/events",
  "html_url": "https://github.com/shap/shap/pull/1073",
  "id": 572446770,
  "node_id": "MDExOlB1bGxSZXF1ZXN0MzgxMTQyNjM5",
  "number": 1073,
  "title": "Pytorch explainer test on gpu mode",
  "user": {
    "login": "rightx2",
    "id": 10606994,
    "node_id": "MDQ6VXNlcjEwNjA2OTk0",
    "avatar_url": "https://avatars.githubusercontent.com/u/10606994?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/rightx2",
    "html_url": "https://github.com/rightx2",
    "followers_url": "https://api.github.com/users/rightx2/followers",
    "following_url": "https://api.github.com/users/rightx2/following{/other_user}",
    "gists_url": "https://api.github.com/users/rightx2/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/rightx2/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/rightx2/subscriptions",
    "organizations_url": "https://api.github.com/users/rightx2/orgs",
    "repos_url": "https://api.github.com/users/rightx2/repos",
    "events_url": "https://api.github.com/users/rightx2/events{/privacy}",
    "received_events_url": "https://api.github.com/users/rightx2/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2020-02-28T00:36:31Z",
  "updated_at": "2020-02-28T00:55:45Z",
  "closed_at": "2020-02-28T00:55:35Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/shap/shap/pulls/1073",
    "html_url": "https://github.com/shap/shap/pull/1073",
    "diff_url": "https://github.com/shap/shap/pull/1073.diff",
    "patch_url": "https://github.com/shap/shap/pull/1073.patch",
    "merged_at": null
  },
  "body": "Since the test codes for `PytorchExlainer` are written only on \"cpu\", I've written the code for testing on \"gpu\"\r\n\r\nAnd I added commits for setting random seed.\r\n\r\nThe reason is that, If I didn't it shows inconsistent results(I run a test 2 times in a row and the first trial didn't pass, but the second was. Below is  history when running a test code on `8e11283` of this PR)\r\n\r\nIf this PR accepted, I will write the same tests on  `_PyTorchGradientExplainer` as well\r\n\r\nThanks\r\n\r\nOS: ubuntu 18.04\r\nDriver Version: 440.33.01 \r\nCUDA Version: 10.2\r\nPython version: 3.7.x\r\nshap version: 0.34.0\r\ntorch version: 1.2.0\r\n\r\n\r\n```\r\n(base) Chois@computer-gpu1:~/shap$ pytest tests/explainers/test_deep.py::test_pytorch_mnist_cnn -s\r\n====================================== test session starts =======================================\r\nplatform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\r\nrootdir: /home/Chois/shap\r\ncollected 1 item\r\n\r\ntests/explainers/test_deep.py Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\r\n9920512it [00:02, 3901093.37it/s]\r\nExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\r\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\r\n32768it [00:00, 52727.96it/s]\r\nExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\r\n1654784it [00:01, 841926.42it/s]\r\nExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\r\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\r\n8192it [00:00, 18812.56it/s]\r\nExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\r\nProcessing...\r\nDone!\r\nRunning test on interim layer\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.091955\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.090444\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.091087\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.091120\r\nRunning test on whole model\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.091641\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.090700\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.092222\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.090528\r\nF\r\n\r\n============================================ FAILURES ============================================\r\n_____________________________________ test_pytorch_mnist_cnn _____________________________________\r\n\r\n    def test_pytorch_mnist_cnn():\r\n        \"\"\"The same test as above, but for pytorch\r\n        \"\"\"\r\n        _skip_if_no_pytorch()\r\n\r\n        import torch, torchvision\r\n        from torchvision import datasets, transforms\r\n        from torch import nn\r\n        from torch.nn import functional as F\r\n        import shap\r\n\r\n        def run_test(train_loader, test_loader, interim, device):\r\n\r\n            class Net(nn.Module):\r\n                def __init__(self):\r\n                    super(Net, self).__init__()\r\n                    # Testing several different activations\r\n                    self.conv_layers = nn.Sequential(\r\n                        nn.Conv2d(1, 10, kernel_size=5),\r\n                        nn.MaxPool2d(2),\r\n                        nn.Tanh(),\r\n                        nn.Conv2d(10, 20, kernel_size=5),\r\n                        nn.ConvTranspose2d(20, 20, 1),\r\n                        nn.AdaptiveAvgPool2d(output_size=(4, 4)),\r\n                        nn.Softplus(),\r\n                    )\r\n                    self.fc_layers = nn.Sequential(\r\n                        nn.Linear(320, 50),\r\n                        nn.BatchNorm1d(50),\r\n                        nn.ReLU(),\r\n                        nn.Linear(50, 10),\r\n                        nn.ELU(),\r\n                        nn.Softmax(dim=1)\r\n                    )\r\n\r\n                def forward(self, x):\r\n                    x = self.conv_layers(x)\r\n                    x = x.view(-1, 320)\r\n                    x = self.fc_layers(x)\r\n                    return x\r\n\r\n            model = Net()\r\n            optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\r\n\r\n            def train(model, device, train_loader, optimizer, epoch, cutoff=2000):\r\n                model.train()\r\n                num_examples = 0\r\n                for batch_idx, (data, target) in enumerate(train_loader):\r\n                    num_examples += target.shape[0]\r\n                    data, target = data.to(device), target.to(device)\r\n                    optimizer.zero_grad()\r\n                    output = model(data)\r\n                    loss = F.mse_loss(output, torch.eye(10)[target].to(device))\r\n                    # loss = F.nll_loss(output, target)\r\n                    loss.backward()\r\n                    optimizer.step()\r\n                    if batch_idx % 10 == 0:\r\n                        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                            epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                                   100. * batch_idx / len(train_loader), loss.item()))\r\n                    if num_examples > cutoff:\r\n                        break\r\n\r\n\r\n            train(model.to(device), device, train_loader, optimizer, 1)\r\n\r\n            next_x, next_y = next(iter(train_loader))\r\n            np.random.seed(0)\r\n            inds = np.random.choice(next_x.shape[0], 20, replace=False)\r\n            if interim:\r\n                e = shap.DeepExplainer(\r\n                    (model.to(device), model.conv_layers[0].to(device)), next_x[inds, :, :, :].to(device)\r\n                )\r\n            else:\r\n                e = shap.DeepExplainer(\r\n                    model.to(device), next_x[inds, :, :, :].to(device)\r\n                )\r\n            test_x, test_y = next(iter(test_loader))\r\n            input_tensor = test_x[:1]\r\n            input_tensor.requires_grad = True\r\n            shap_values = e.shap_values(input_tensor)\r\n\r\n            model = model.to(device)\r\n            model.eval()\r\n            model.zero_grad()\r\n            with torch.no_grad():\r\n                test_outputs = model(test_x[:1].to(device))\r\n                next_outputs = model(next_x[inds, :].to(device))\r\n                diff = (test_outputs - next_outputs).detach().cpu().numpy().mean(0)\r\n            sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\r\n            d = np.abs(sums - diff).sum()\r\n            assert d / np.abs(diff).sum() < 0.001, \"Sum of SHAP values does not match difference! %f\" % (\r\n                    d / np.abs(diff).sum())\r\n\r\n        batch_size = 128\r\n        root_dir = 'mnist_data'\r\n\r\n        train_loader = torch.utils.data.DataLoader(\r\n            datasets.MNIST(root_dir, train=True, download=True,\r\n                           transform=transforms.Compose([\r\n                               transforms.ToTensor(),\r\n                               transforms.Normalize((0.1307,), (0.3081,))\r\n                           ])),\r\n            batch_size=batch_size, shuffle=True)\r\n        test_loader = torch.utils.data.DataLoader(\r\n            datasets.MNIST(root_dir, train=False, download=True,\r\n                           transform=transforms.Compose([\r\n                               transforms.ToTensor(),\r\n                               transforms.Normalize((0.1307,), (0.3081,))\r\n                           ])),\r\n            batch_size=batch_size, shuffle=True)\r\n\r\n        print ('Running test on interim layer')\r\n        run_test(train_loader, test_loader, interim=True, device=\"cpu\")\r\n        if torch.cuda.is_available():\r\n            run_test(train_loader, test_loader, interim=True, device=\"cuda:0\")\r\n        print ('Running test on whole model')\r\n        run_test(train_loader, test_loader, interim=False, device=\"cpu\")\r\n        if torch.cuda.is_available():\r\n>           run_test(train_loader, test_loader, interim=True, device=\"cuda:0\")\r\n\r\ntests/explainers/test_deep.py:321:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ntrain_loader = <torch.utils.data.dataloader.DataLoader object at 0x7fa2c3718f50>\r\ntest_loader = <torch.utils.data.dataloader.DataLoader object at 0x7fa2c13a6c10>, interim = True\r\ndevice = 'cuda:0'\r\n\r\n    def run_test(train_loader, test_loader, interim, device):\r\n\r\n        class Net(nn.Module):\r\n            def __init__(self):\r\n                super(Net, self).__init__()\r\n                # Testing several different activations\r\n                self.conv_layers = nn.Sequential(\r\n                    nn.Conv2d(1, 10, kernel_size=5),\r\n                    nn.MaxPool2d(2),\r\n                    nn.Tanh(),\r\n                    nn.Conv2d(10, 20, kernel_size=5),\r\n                    nn.ConvTranspose2d(20, 20, 1),\r\n                    nn.AdaptiveAvgPool2d(output_size=(4, 4)),\r\n                    nn.Softplus(),\r\n                )\r\n                self.fc_layers = nn.Sequential(\r\n                    nn.Linear(320, 50),\r\n                    nn.BatchNorm1d(50),\r\n                    nn.ReLU(),\r\n                    nn.Linear(50, 10),\r\n                    nn.ELU(),\r\n                    nn.Softmax(dim=1)\r\n                )\r\n\r\n            def forward(self, x):\r\n                x = self.conv_layers(x)\r\n                x = x.view(-1, 320)\r\n                x = self.fc_layers(x)\r\n                return x\r\n\r\n        model = Net()\r\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\r\n\r\n        def train(model, device, train_loader, optimizer, epoch, cutoff=2000):\r\n            model.train()\r\n            num_examples = 0\r\n            for batch_idx, (data, target) in enumerate(train_loader):\r\n                num_examples += target.shape[0]\r\n                data, target = data.to(device), target.to(device)\r\n                optimizer.zero_grad()\r\n                output = model(data)\r\n                loss = F.mse_loss(output, torch.eye(10)[target].to(device))\r\n                # loss = F.nll_loss(output, target)\r\n                loss.backward()\r\n                optimizer.step()\r\n                if batch_idx % 10 == 0:\r\n                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                        epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                               100. * batch_idx / len(train_loader), loss.item()))\r\n                if num_examples > cutoff:\r\n                    break\r\n\r\n\r\n        train(model.to(device), device, train_loader, optimizer, 1)\r\n\r\n        next_x, next_y = next(iter(train_loader))\r\n        np.random.seed(0)\r\n        inds = np.random.choice(next_x.shape[0], 20, replace=False)\r\n        if interim:\r\n            e = shap.DeepExplainer(\r\n                (model.to(device), model.conv_layers[0].to(device)), next_x[inds, :, :, :].to(device)\r\n            )\r\n        else:\r\n            e = shap.DeepExplainer(\r\n                model.to(device), next_x[inds, :, :, :].to(device)\r\n            )\r\n        test_x, test_y = next(iter(test_loader))\r\n        input_tensor = test_x[:1]\r\n        input_tensor.requires_grad = True\r\n        shap_values = e.shap_values(input_tensor)\r\n\r\n        model = model.to(device)\r\n        model.eval()\r\n        model.zero_grad()\r\n        with torch.no_grad():\r\n            test_outputs = model(test_x[:1].to(device))\r\n            next_outputs = model(next_x[inds, :].to(device))\r\n            diff = (test_outputs - next_outputs).detach().cpu().numpy().mean(0)\r\n        sums = np.array([shap_values[i].sum() for i in range(len(shap_values))])\r\n        d = np.abs(sums - diff).sum()\r\n>       assert d / np.abs(diff).sum() < 0.001, \"Sum of SHAP values does not match difference! %f\" % (\r\n                d / np.abs(diff).sum())\r\nE       AssertionError: Sum of SHAP values does not match difference! 0.001130\r\nE       assert (1.365514702511561e-05 / 0.012087966) < 0.001\r\nE        +  where 0.012087966 = <built-in method sum of numpy.ndarray object at 0x7fa322c63f80>()\r\nE        +    where <built-in method sum of numpy.ndarray object at 0x7fa322c63f80> = array([0.00034282, 0.00224469, 0.00011375, 0.00256004, 0.00051804,\\n       0.00165695, 0.00027921, 0.00112554, 0.00165977, 0.00158717],\\n      dtype=float32).sum\r\nE        +      where array([0.00034282, 0.00224469, 0.00011375, 0.00256004, 0.00051804,\\n       0.00165695, 0.00027921, 0.00112554, 0.00165977, 0.00158717],\\n      dtype=float32) = <ufunc 'absolute'>(array([ 0.00034282, -0.00224469, -0.00011375, -0.00256004,  0.00051804,\\n        0.00165695,  0.00027921, -0.00112554,  0.00165977,  0.00158717],\\n      dtype=float32))\r\nE        +        where <ufunc 'absolute'> = np.abs\r\n\r\ntests/explainers/test_deep.py:293: AssertionError\r\n======================================== warnings summary ========================================\r\n/home/Chois/miniconda3/lib/python3.7/site-packages/nose/importer.py:12\r\n  /home/Chois/miniconda3/lib/python3.7/site-packages/nose/importer.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    from imp import find_module, load_module, acquire_lock, release_lock\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n================================ 1 failed, 13 warnings in 15.12s =================================\r\n(base) Chois@computer-gpu1:~/shap$ pytest tests/explainers/test_deep.py::test_pytorch_mnist_cnn -s\r\n====================================== test session starts =======================================\r\nplatform linux -- Python 3.7.4, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\r\nrootdir: /home/Chois/shap\r\ncollected 1 item\r\n\r\ntests/explainers/test_deep.py Running test on interim layer\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.091529\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.092290\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.090987\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.091052\r\nRunning test on whole model\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.092100\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.090334\r\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 0.091841\r\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.090477\r\n.\r\n\r\n======================================== warnings summary ========================================\r\n/home/Chois/miniconda3/lib/python3.7/site-packages/nose/importer.py:12\r\n  /home/Chois/miniconda3/lib/python3.7/site-packages/nose/importer.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    from imp import find_module, load_module, acquire_lock, release_lock\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\r\n\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\ntests/explainers/test_deep.py::test_pytorch_mnist_cnn\r\n  numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n================================= 1 passed, 13 warnings in 8.71s =================================\r\n(base) Chois@computer-gpu1:~/shap$\r\n```\r\n",
  "closed_by": {
    "login": "rightx2",
    "id": 10606994,
    "node_id": "MDQ6VXNlcjEwNjA2OTk0",
    "avatar_url": "https://avatars.githubusercontent.com/u/10606994?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/rightx2",
    "html_url": "https://github.com/rightx2",
    "followers_url": "https://api.github.com/users/rightx2/followers",
    "following_url": "https://api.github.com/users/rightx2/following{/other_user}",
    "gists_url": "https://api.github.com/users/rightx2/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/rightx2/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/rightx2/subscriptions",
    "organizations_url": "https://api.github.com/users/rightx2/orgs",
    "repos_url": "https://api.github.com/users/rightx2/repos",
    "events_url": "https://api.github.com/users/rightx2/events{/privacy}",
    "received_events_url": "https://api.github.com/users/rightx2/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1073/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1073/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
