{
  "url": "https://api.github.com/repos/shap/shap/issues/696",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/696/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/696/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/696/events",
  "html_url": "https://github.com/shap/shap/issues/696",
  "id": 466456052,
  "node_id": "MDU6SXNzdWU0NjY0NTYwNTI=",
  "number": 696,
  "title": "Should global shap values depend on the variance of the feature?",
  "user": {
    "login": "jorgecarleitao",
    "id": 2772607,
    "node_id": "MDQ6VXNlcjI3NzI2MDc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/2772607?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/jorgecarleitao",
    "html_url": "https://github.com/jorgecarleitao",
    "followers_url": "https://api.github.com/users/jorgecarleitao/followers",
    "following_url": "https://api.github.com/users/jorgecarleitao/following{/other_user}",
    "gists_url": "https://api.github.com/users/jorgecarleitao/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/jorgecarleitao/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jorgecarleitao/subscriptions",
    "organizations_url": "https://api.github.com/users/jorgecarleitao/orgs",
    "repos_url": "https://api.github.com/users/jorgecarleitao/repos",
    "events_url": "https://api.github.com/users/jorgecarleitao/events{/privacy}",
    "received_events_url": "https://api.github.com/users/jorgecarleitao/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 6,
  "created_at": "2019-07-10T18:31:51Z",
  "updated_at": "2020-02-08T04:40:48Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "I am struggling to make sense of how \"global feature importance\" is computed, and why it should depend on the variance of a feature.\r\n\r\nSpecifically, in the source code, \"global shap values\" is computed with the formula: `np.abs(shap).mean(0)` (see [here](https://github.com/slundberg/shap/blob/6bf9df73180b621d8dd5be088c767d7bf0262e2a/shap/plots/summary.py#L382), which amounts to compute the average of the absolute value of the shap values).\r\n\r\nLet's take the following example: It basically sets some random variables, relates them with y, fits a lasso model, and computes the respective shap values and global shap values.\r\n\r\n```\r\nimport numpy as np\r\nfrom shap import LinearExplainer\r\nfrom sklearn.linear_model import LassoCV\r\n\r\n\r\nnp.random.seed(1)\r\n\r\na1 = np.random.normal(size=500, scale=1)\r\na2 = np.random.normal(size=500, scale=1)\r\na3 = np.random.normal(size=500, scale=1)\r\n\r\nx = np.array([a1, a2, a3]).T\r\ny = 100 * a1 + 20 * a2 + np.random.normal(size=500, scale=0.01)\r\n\r\nmodel = LassoCV(cv=5)\r\n\r\nmodel.fit(x, y)\r\n\r\nexplainer = LinearExplainer(model, x)\r\nshap = explainer.shap_values(x)\r\n\r\nvalues = np.abs(shap).mean(0)\r\nprint(values)\r\n# [78.9061823  15.19658609  0.        ]\r\n```\r\n\r\nThe issue for me is that these values depend on the variance of `a{i}`.\r\n\r\nWhy? `mean(abs(shap))` will on average be equal to `np.sqrt(2/np.pi) * np.std(shap, axis=0)` (see [expectation of half-normal distribution](https://en.wikipedia.org/wiki/Half-normal_distribution)). In this case, since we are in the linear models and `a{i}` is centered, we get that `std(shap) = std((x - m(x)) * c) = std(x) * c`. We can confirm this by also printing\r\n\r\n     print(np.sqrt(2/np.pi) * np.std(shap, axis=0))\r\n     print(np.sqrt(2/np.pi) * np.std(x, axis=0) * model.coef_)\r\n\r\nTherefore, changing the initial scale of, e.g. `a2`, we get a different importance. Indeed, changing the line above to \r\n\r\n     a2 = np.random.normal(size=500, scale=10)\r\n\r\nleads to the output\r\n\r\n     [ 77.43738977 152.6175833    0.        ]\r\n\r\ni.e. `a2` is now more important than `a1`, even though we only increased its variance. Shouldn't the global feature importance be independent of the variance of the feature?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/696/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/696/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
