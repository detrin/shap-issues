{
  "url": "https://api.github.com/repos/shap/shap/issues/747",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/747/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/747/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/747/events",
  "html_url": "https://github.com/shap/shap/issues/747",
  "id": 480832281,
  "node_id": "MDU6SXNzdWU0ODA4MzIyODE=",
  "number": 747,
  "title": "Intuitiveness of SHAP explanation based on example - some doubts and question about possible solution",
  "user": {
    "login": "MichalPorwisz",
    "id": 23621628,
    "node_id": "MDQ6VXNlcjIzNjIxNjI4",
    "avatar_url": "https://avatars.githubusercontent.com/u/23621628?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/MichalPorwisz",
    "html_url": "https://github.com/MichalPorwisz",
    "followers_url": "https://api.github.com/users/MichalPorwisz/followers",
    "following_url": "https://api.github.com/users/MichalPorwisz/following{/other_user}",
    "gists_url": "https://api.github.com/users/MichalPorwisz/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/MichalPorwisz/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/MichalPorwisz/subscriptions",
    "organizations_url": "https://api.github.com/users/MichalPorwisz/orgs",
    "repos_url": "https://api.github.com/users/MichalPorwisz/repos",
    "events_url": "https://api.github.com/users/MichalPorwisz/events{/privacy}",
    "received_events_url": "https://api.github.com/users/MichalPorwisz/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2019-08-14T18:46:35Z",
  "updated_at": "2019-08-15T04:44:28Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I have some doubts about intuitiveness of SHAP explanation in some of cases. Let me explain my point on an example.\r\n\r\nLet's take a classifier trained on a Titanic dataset (predicting chance of survival). For simplicity, let's only take into account 2 features: age and sex. Let our sample have age=2 and sex=0, meaning a two-year old boy.\r\n\r\nThe fact is that children and women had higher chances of survival that adult men. Let's assume that there was no statistically relevant difference between boys and girls. A good classifier will be able to follow this pattern. Thus, the probability of survival for this sample will be higher than for an average sample. Main effect of \"sex\" will be negative, as there were more adult men than boys in the dataset, so that is what will be produced by integrating over background dataset. Interaction of age and sex will be positive, but only to counter the value of negative main effect, as the interaction itself is neutral (boys have similar survival rate as girls). The interaction will be split between sex and age, so actually the SHAP value for sex will still be negative.\r\n\r\nEven though SHAP values for those features will be \"fair\", it seems to me that the explanation based on them won't be intuitive, because it will say that sex has negative impact on the prediction value. One could also provide the values of main effects and interaction value, but then again this may not be obvious that sex+age interaction only counters main effect of sex and that overall it's only important for this sample, that the passenger is a child.\r\n\r\nAn alternative explanation could be:\r\n\"Low age (being a child) contributes to higher chance of survival. Given low age, value of sex (being a boy) doesn't change he probability of survival.\"\r\nThis explanation doesn't change the main effects and interaction value coming from SHAP, it just \"rearranges\" those values - it presents main effect of age and then presents total result of main effect of sex and sex+age interaction. In other words, we choose to combine interaction value with opposite/contradictory main effect.\r\n\r\nCombining age main effect and sex+age interaction is less intuitive, as this would say: \"Value of sex (being a men) contributes to lower chance of survival. Given that value of sex, low age (being a child) makes the chance of survival increase (by more than was decreased by value of sex)\". This explanation doesn't actually highlight the fact, that part of effect of age (interaction with sex) is only something that reduces negative main effect of sex.\r\n\r\nSimilar logic would hold, if feature _i_ has positive main effect, feature _j_ has negative main effect _x_ and their interaction would be positive _y_. In similar spirit, I would propose to provide the explanation that feature _i_ contributed _x_ to the prediction, and then \"feature _j_ given _i_\" contributed _y + x_. Either _y + x > 0_ meaning interaction nullifies the main effect (kind of like titanic example) or _y + x < 0_ - meaning feature _j_ still has negative impact, but it is reduced by the interaction.\r\n\r\nIn case interaction value is opposite to both main effects, one could sum their SHAP values and say that together these features contribute this sum. Clearly these features together lead to a completely opposite conclusion than separately, so it seems to me, that it's more intuitive to present them together.\r\n\r\nThe argument for this kind of explanations could be based on the way we explain decisions in natural language. For example one could say - \"High income contributes positively to chance of loan approval\" - I would say that speaker refers to the main effect of high income here. If one wanted to mean high income as well as it's interactions with all other factors (SHAP value), it would be stated explicitly.\r\nAnother example - \"The fact that subject has 180 Systolic BP contributes only slightly to higher to the death risk, given that subject is 70 years old\" - here I would say that speaker refers to the sum of Systolic BP main effect and Systolic BP+Age interaction value (in both directions - so i+j and j+i).\r\n\r\nSo it seems that indeed it's more intuitive to provide either main effects or main effect+ total interaction value (both _[i,j]_ and _[j,i]_), instead of providing all main effects and interactions separately or splitting the interaction between both features and providing SHAP values.\r\n\r\nThis is not very formal and I have presented it on a very simplified example. In reality, features may have multiple relevant interactions (we would maybe need to split the main effect between opposite interactions). Still, I would very much like to get your opinion, if there is any merit in my logic that would be worth pursuing. Let me know if there are any flaws in my logic or if, for some reason, you would think that there is not much value of that proposal in real life applications. Thanks for your time.\r\n\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/747/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/747/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
