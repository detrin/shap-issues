{
  "url": "https://api.github.com/repos/shap/shap/issues/967",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/967/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/967/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/967/events",
  "html_url": "https://github.com/shap/shap/issues/967",
  "id": 541357336,
  "node_id": "MDU6SXNzdWU1NDEzNTczMzY=",
  "number": 967,
  "title": "Issue with highly-codependent variables with data generated by simple linear equation",
  "user": {
    "login": "parrt",
    "id": 178777,
    "node_id": "MDQ6VXNlcjE3ODc3Nw==",
    "avatar_url": "https://avatars.githubusercontent.com/u/178777?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/parrt",
    "html_url": "https://github.com/parrt",
    "followers_url": "https://api.github.com/users/parrt/followers",
    "following_url": "https://api.github.com/users/parrt/following{/other_user}",
    "gists_url": "https://api.github.com/users/parrt/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/parrt/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/parrt/subscriptions",
    "organizations_url": "https://api.github.com/users/parrt/orgs",
    "repos_url": "https://api.github.com/users/parrt/repos",
    "events_url": "https://api.github.com/users/parrt/events{/privacy}",
    "received_events_url": "https://api.github.com/users/parrt/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2019-12-21T18:39:23Z",
  "updated_at": "2019-12-27T20:45:44Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "howdy!  I have a simple linear equation that generates data but with a wrinkle; I've copied x1 into x3 and added some zero-centered low-amplitude noise to make them codependent but non-identical:\r\n\r\n```\r\ny = x_1 + x_2 + x_3 + 100 where x_3 = x_1 + noise, x_1, x_2 ~ U(0,10)\r\n```\r\n\r\nIgnoring x3 copy thing for the moment, we'd expect partial derivatives of y w.r.t. xi to be 1.0.  OLS indeed shows coefficients of 1.0 (of course with high variance on the betas).  SHAP `LinearExplainer` gives mean(abs(shap)) of [2.57113498 2.46506859 2.56234981], so equal weight to each coefficient.  By the way, should these be 1.0 also?\r\n\r\nNow, x1 and x3 are not independent but I argue we should be able to tease apart x3's contribution as OLS and OLS shap are doing. Imagine we have enough data so we can find pairs of records with equal x1 and x3 values; they only differ in x3 due to noise:\r\n\r\n<img width=\"211\" alt=\"Screen Shot 2019-12-14 at 10 50 20 AM\" src=\"https://user-images.githubusercontent.com/178777/71312166-9f438100-23dc-11ea-85dc-00e8be777170.png\">\r\n\r\nSeems like the change in y due solely to x3 (rise/run) would be:\r\n\r\n```\r\n(22+e2) - (22+e1)\r\n------------------------\r\n(9+e2) - (9+e1)\r\n```\r\n\r\nwhich reduces to \r\n\r\n```\r\ne2 - e1\r\n---------\r\ne2 - e1\r\n```\r\n\r\nor 1.0 for any e1, e2 noise and any pair of stratified records.\r\n\r\nTurning to a random forest model, we see x1 and x3 contributions bounce all over the place. As you explained in email, an RF has equally likely chances of using x1 or x3 when building decision trees so we would expect high variance between x1 and x3 contribution. However, wouldn't the expectation of x1 and x3 be 1.0 (or some fixed value...can't remember), just with high variance?  I ran a long experiment (50 trials) and they do seem to converge to the expectation.\r\n\r\nFor sure this example also highlights how one can get very different answers depending on the chosen model.\r\n\r\nI enclose self-contained code to show OLS vs RF results.\r\n\r\nThanks for your attention!\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.linear_model import LinearRegression\r\nimport statsmodels.api as sm\r\n\r\nimport shap\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndef synthetic_poly_dup_data(n):\r\n    p = 3 # x1, x2, x3\r\n    df = pd.DataFrame()\r\n    for i in range(p):\r\n        df[f'x{i + 1}'] = np.random.random_sample(size=n) * 10\r\n    # copy x1 into x3 with zero-centered noise\r\n    df['x3'] = df['x1'] + np.random.random_sample(size=n)-0.5\r\n    yintercept = 100\r\n    df['y'] = np.sum(df, axis=1) + yintercept\r\n    terms = [f\"x_{i+1}\" for i in range(p)] + [f\"{yintercept:.0f}\"]\r\n    eqn = \"y = \" + ' + '.join(terms) + \" where x_3 = x_1 + noise, x_1, x_2 ~ U(0,10)\"\r\n    return df, eqn\r\n\r\ndf, eqn = synthetic_poly_dup_data(1000)\r\nX = df.drop('y', axis=1)\r\ny = df['y']\r\n\r\nprint(eqn)\r\n\r\n# Use OLS to get coeff and then LinearExplainer\r\n\r\nlm = LinearRegression()\r\nlm.fit(X,y)\r\nprint(\"OLS coeff\", lm.coef_)\r\ny_pred = lm.predict(X)\r\nprint(f\"OLS Training MSE {np.mean((y - y_pred) ** 2):.5f}\")\r\n\r\nexplainer = shap.LinearExplainer(lm, X, feature_dependence='independent')\r\nshap_values = explainer.shap_values(X)\r\nshapimp = np.mean(np.abs(shap_values), axis=0)\r\nprint(\"OLS SHAP importances\", shapimp)\r\n\r\nbeta_stderr = sm.OLS(y, X).fit().bse\r\nprint(f\"linear model coefficient stderr {beta_stderr.values}\\n(high variance on x1 and x3 as expected)\")\r\n\r\n# Try now with RF model and TreeExplainer\r\n\r\nfor i in range(5): # try 5 times to see variance in x_1, x_3\r\n    print(f\"Trial {i+1}\")\r\n    rf = RandomForestRegressor(n_estimators=20)\r\n    rf.fit(X, y)\r\n    print(f\"RF Training MSE {np.mean((y - rf.predict(X)) ** 2):.5f}\")\r\n    explainer = shap.TreeExplainer(rf, data=X, feature_perturbation='interventional')\r\n    shap_values = explainer.shap_values(X)\r\n    shapimp = np.mean(np.abs(shap_values), axis=0)\r\n    print(\"RF SHAP importances\", shapimp)\r\n```\r\n\r\nOUTPUT:\r\n\r\n```\r\ny = x_1 + x_2 + x_3 + 100 where x_3 = x_1 + noise, x_1, x_2 ~ U(0,10)\r\nOLS coeff [1. 1. 1.]\r\nOLS Training MSE 0.00000\r\nOLS SHAP importances [2.57113498 2.46506859 2.56234981]\r\nlinear model coefficient stderr [4.10717122 0.31065979 4.1116481 ]\r\n(high variance on x1 and x3 as expected)\r\nTrial 1\r\nRF Training MSE 0.01661\r\nRF SHAP importances [2.78986432 2.45207256 2.35204797]\r\nTrial 2\r\nRF Training MSE 0.01566\r\nRF SHAP importances [2.32197286 2.42069104 2.81279345]\r\nTrial 3\r\nRF Training MSE 0.01574\r\nRF SHAP importances [2.69656699 2.40900263 2.44039671]\r\nTrial 4\r\nRF Training MSE 0.01584\r\nRF SHAP importances [2.95876692 2.42191879 2.18118951]\r\nTrial 5\r\nRF Training MSE 0.01641\r\nRF SHAP importances [2.48588282 2.44207147 2.64593201]\r\n```",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/967/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/967/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
