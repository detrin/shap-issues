{
  "url": "https://api.github.com/repos/shap/shap/issues/1204",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1204/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1204/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1204/events",
  "html_url": "https://github.com/shap/shap/issues/1204",
  "id": 615296551,
  "node_id": "MDU6SXNzdWU2MTUyOTY1NTE=",
  "number": 1204,
  "title": "Loky crashes when calculating shap values with a large scikit-learn RF model",
  "user": {
    "login": "akuhnregnier",
    "id": 20064887,
    "node_id": "MDQ6VXNlcjIwMDY0ODg3",
    "avatar_url": "https://avatars.githubusercontent.com/u/20064887?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/akuhnregnier",
    "html_url": "https://github.com/akuhnregnier",
    "followers_url": "https://api.github.com/users/akuhnregnier/followers",
    "following_url": "https://api.github.com/users/akuhnregnier/following{/other_user}",
    "gists_url": "https://api.github.com/users/akuhnregnier/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/akuhnregnier/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/akuhnregnier/subscriptions",
    "organizations_url": "https://api.github.com/users/akuhnregnier/orgs",
    "repos_url": "https://api.github.com/users/akuhnregnier/repos",
    "events_url": "https://api.github.com/users/akuhnregnier/events{/privacy}",
    "received_events_url": "https://api.github.com/users/akuhnregnier/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2020-05-10T01:51:42Z",
  "updated_at": "2022-03-18T20:34:37Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "## Problem - cannot parallelise `shap_values()` using joblib (loky)\r\n\r\nWhen using the [loky](https://loky.readthedocs.io/en/stable/) parallel backend from [Joblib](https://joblib.readthedocs.io/en/latest/) with a scikit-learn `RandomForestRegressor`, calling the `TreeExplainer.shap_values()` method crashes the worker processes:\r\n\r\n> joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {SIGSEGV(-11)}\r\n\r\nHowever, this _only_ happens when passing an existing `TreeExplainer` instance to the worker processes, for a model above a certain size/complexity. \r\n\r\nThe module that `loky` uses for serialisation doesn't seem to matter either, as the same error occurs with or without `cloudpickle`, which is used by `loky` by default, but can be turned off using:\r\n```\r\nfrom joblib.externals.loky import set_loky_pickler\r\nset_loky_pickler('pickle')\r\n```\r\n\r\nInterestingly, using the joblib `multiprocessing` backend for the same cases where the above error occurs (using the loky backend) results in a situation where the worker processes never terminate, without any CPU utilisation.\r\n\r\n### Workarounds\r\n\r\n#### Create the `TreeExplainer` within each worker function\r\n\r\nGiving the workers the `RandomForestRegressor` instance and then re-creating a `TreeExplainer` within each worker function resolves the issue (as far as I can tell).\r\n\r\n#### Don't use loky\r\n\r\nAdditionally, other means of initiating multi-processing don't seem to be affected by this (unpickled vs. newly created `TreeExplainer`) at all; these work as expected for example: \r\n - `multiprocessing.Pool`\r\n - dask distributed [`LocalCluster`](https://docs.dask.org/en/latest/setup/single-distributed.html)\r\n\r\nThe same is true for threading, but there isn't any speed-up of course.\r\n\r\n### Failing cases\r\n\r\nI have only tested this using `RandomForestRegressor`.\r\nI observed the error for a training dataset with 1,000,000 rows and 2 or 20 columns, and a `RandomForestRegressor` with `max_depth=12`, and `n_estimators=200`:\r\n```\r\nRandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\r\n                      max_depth=12, max_features='auto', max_leaf_nodes=None,\r\n                      max_samples=None, min_impurity_decrease=0.0,\r\n                      min_impurity_split=None, min_samples_leaf=1,\r\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\r\n                      n_estimators=200, n_jobs=None, oob_score=False,\r\n                      random_state=None, verbose=0, warm_start=False)\r\n```\r\nDepending on the training dataset `X` (uniformly distributed random data), the number of leaves in the 200 trees varied:\r\n - 1,000,000 rows by 2 columns: (mean ± std) 2531 ± 82, min. 2296, max. 2729 leaves\r\n - 1,000,000 rows by 20 columns: (mean ± std) 4080 ± 5, min. 4063, max. 4094 leaves\r\n\r\nThe target variable `y` for the regression was always a simple linear function of the input features.\r\n\r\n### Example code\r\n\r\nA short script to recreate the error:\r\n```\r\nimport os\r\nfrom functools import partial\r\nfrom multiprocessing import Pool\r\nfrom time import time\r\n\r\nimport numpy as np\r\nfrom joblib import Parallel, delayed, parallel_backend\r\nfrom scipy.stats import describe\r\nfrom shap import TreeExplainer\r\nfrom sklearn.ensemble import RandomForestRegressor\r\n\r\n\r\ndef get_shap_values(explainer, X, y, samples, est=None):\r\n    \"\"\"Using a saved copy of a model, calculate SHAP values.\r\n    Args:\r\n        explainer (shap TreeExplainer):\r\n        X (array-like): Training data.\r\n        y (array-like): Training target.\r\n        samples (int): Number of samples to compute SHAP values for.\r\n        est (Tree-Estimator): If given, a new TreeExplainer will be constructed and\r\n            used, meaning that `explainer` is not used.\r\n    \"\"\"\r\n    if est is not None:\r\n        print(\"Constructing new explainer\")\r\n        # Construct a new TreeExplainer.\r\n        explainer = TreeExplainer(est, feature_perturbation=\"tree_path_dependent\")\r\n    else:\r\n        print(\"Re-using explainer\")\r\n\r\n    start = time()\r\n    values = explainer.shap_values(X[:samples], y[:samples])\r\n    shap_time = time() - start\r\n    return (\r\n        values,\r\n        \" \".join(map(str, (\"PID:\", os.getpid(), \"time taken for SHAP\", shap_time))),\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    np.random.seed(1)\r\n\r\n    N = int(1e6)\r\n    col = 2\r\n    n_estimators = 200\r\n    max_depth = 12\r\n    cores = 4\r\n    pass_explainer = True  # True triggers the error.\r\n\r\n    X = np.random.random((N, col))\r\n    est = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\r\n\r\n    factors = np.linspace(0, 1, X.shape[1])\r\n    y = sum(\r\n        X[:, i] * factor + np.random.random(X.shape[0]) * 0.8\r\n        for i, factor in enumerate(factors)\r\n    )\r\n    feature_names = [f\"feature {i}\" for i in range(len(factors))]\r\n\r\n    print(\"Fitting model (this may take a while)...\")\r\n    with parallel_backend(\"threading\", n_jobs=cores):\r\n        est.fit(X, y)\r\n\r\n    print(est)\r\n    print(describe([tree.get_n_leaves() for tree in est.estimators_]))\r\n\r\n    if pass_explainer:\r\n        explainer = TreeExplainer(est, feature_perturbation=\"tree_path_dependent\")\r\n        est = None\r\n    else:\r\n        explainer = None\r\n\r\n    shap_func = partial(get_shap_values, explainer, X, y, 10, est=est)\r\n\r\n    # The simplest case - no multi-processing.\r\n    results = (shap_func(),)\r\n    print(\"No multiprocessing - done.\")\r\n\r\n    # Using the python multiprocessing backend works.\r\n    with Pool(processes=cores) as pool:\r\n        async_results = [pool.apply_async(shap_func) for i in range(cores)]\r\n        results = [async_result.get() for async_result in async_results]\r\n    print(\"Multiprocessing Pool - done.\")\r\n\r\n    # The failing case - the loky executor.\r\n    with parallel_backend(\"loky\", n_jobs=cores):\r\n        results = Parallel()(delayed(shap_func)() for i in range(cores))\r\n    print(\"Loky - done.\")\r\n```\r\n\r\nPlease see https://gist.github.com/akuhnregnier/d2af98b4efea0eadcc9ee47ef38eefeb for a short command line script to re-create the error:\r\n```\r\npython multi_proc_shap.py -N 1000000 --cols 20 --cores 2 --estimators 200 --max-depth 12 --backend loky --pass-explainer\r\n```\r\nNote that this will store the training data and the trained model in the `tempfile.gettempdir()` temporary directory (in the `shap_multi_data`) folder to speed up re-running of tests with different multi-processing methods for the same model.\r\n\r\nPlease see this file for package versions used (the issue persists after installing `shap` from the `master` branch): https://gist.github.com/akuhnregnier/d2af98b4efea0eadcc9ee47ef38eefeb#file-environment-yml\r\n\r\nThe code was run on:\r\n```\r\nLinux 5.4.0-29-generic #33-Ubuntu SMP Wed Apr 29 14:32:27 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\nI'm not sure what is going wrong here, and if it is exclusively related to `Joblib`, or to `shap` as well. If you think this issue doesn't belong here but rather with `Joblib`, feel free to close it. \r\n\r\nA short caveat in the documentation to alert users to this quirk might be useful.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1204/reactions",
    "total_count": 1,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 1
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1204/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
