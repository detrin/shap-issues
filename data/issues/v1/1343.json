{
  "url": "https://api.github.com/repos/shap/shap/issues/1343",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1343/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1343/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1343/events",
  "html_url": "https://github.com/shap/shap/issues/1343",
  "id": 672155514,
  "node_id": "MDU6SXNzdWU2NzIxNTU1MTQ=",
  "number": 1343,
  "title": "Front Page DeepExplainer MNIST tutorial is broken",
  "user": {
    "login": "palatyle",
    "id": 66335926,
    "node_id": "MDQ6VXNlcjY2MzM1OTI2",
    "avatar_url": "https://avatars.githubusercontent.com/u/66335926?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/palatyle",
    "html_url": "https://github.com/palatyle",
    "followers_url": "https://api.github.com/users/palatyle/followers",
    "following_url": "https://api.github.com/users/palatyle/following{/other_user}",
    "gists_url": "https://api.github.com/users/palatyle/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/palatyle/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/palatyle/subscriptions",
    "organizations_url": "https://api.github.com/users/palatyle/orgs",
    "repos_url": "https://api.github.com/users/palatyle/repos",
    "events_url": "https://api.github.com/users/palatyle/events{/privacy}",
    "received_events_url": "https://api.github.com/users/palatyle/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 486901329,
      "node_id": "MDU6TGFiZWw0ODY5MDEzMjk=",
      "url": "https://api.github.com/repos/shap/shap/labels/bug",
      "name": "bug",
      "color": "ee0701",
      "default": true,
      "description": "Indicates an unexpected problem or unintended behaviour"
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2020-08-03T15:24:32Z",
  "updated_at": "2021-06-17T23:58:37Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi all,\r\n\r\nI'm trying to run the DeepExplainer MNIST tutorial, but am running into errors. I've tried this on both Mac and Windows with the same errors. Everything works up until this line:\r\n\r\n`shap_values = e.shap_values(x_test[1:5])`\r\n\r\nWhere I get the following string of errors: \r\n\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2638       with c_api_util.tf_buffer() as buf:\r\n-> 2639         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2640         data = c_api.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'dense_2/Softmax' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    397     try:\r\n--> 398       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    399       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2642       # Convert to ValueError for backwards compatibility.\r\n-> 2643       raise ValueError(str(e))\r\n   2644     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'dense_2/Softmax' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2638       with c_api_util.tf_buffer() as buf:\r\n-> 2639         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2640         data = c_api.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'gradients_1/dense_2/Softmax_grad/truediv' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    397     try:\r\n--> 398       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    399       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2642       # Convert to ValueError for backwards compatibility.\r\n-> 2643       raise ValueError(str(e))\r\n   2644     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'gradients_1/dense_2/Softmax_grad/truediv' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1863   try:\r\n-> 1864     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1865   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimension 1 in both shapes must be equal, but are 10 and 1. Shapes are [?,10] and [?,1]. for 'gradients_1/dense_2/Softmax_grad/gradients/gradients_1/dense_2/Softmax_grad/truediv_grad/Select_1' (op: 'Select') with input shapes: [?,1], [?,10], [?,10].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-f9a2d473c781> in <module>\r\n      1 # ...or pass tensors directly\r\n      2 # e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\r\n----> 3 shap_values = e.shap_values(x_test[1:5])\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/__init__.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    117         were chosen as \"top\".\r\n    118         \"\"\"\r\n--> 119         return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    302                 # run attribution computation graph\r\n    303                 feature_ind = model_output_ranks[j,i]\r\n--> 304                 sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)\r\n    305 \r\n    306                 # assign the attributions to the right part of the output arrays\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in phi_symbolic(self, i)\r\n    228                     return tf.gradients(out, self.model_inputs)\r\n    229 \r\n--> 230                 self.phi_symbolics[i] = self.execute_with_overridden_gradients(anon)\r\n    231             else:\r\n    232                 @tf.function\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in execute_with_overridden_gradients(self, f)\r\n    395         # define the computation graph for the attribution values using a custom gradient-like computation\r\n    396         try:\r\n--> 397             out = f()\r\n    398         finally:\r\n    399             # reinstate the backpropagatable check\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in anon()\r\n    226                 def anon():\r\n    227                     out = self.model_output[:,i] if self.multi_output else self.model_output\r\n--> 228                     return tf.gradients(out, self.model_inputs)\r\n    229 \r\n    230                 self.phi_symbolics[i] = self.execute_with_overridden_gradients(anon)\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    729                 # functions.\r\n    730                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 731                                          lambda: grad_fn(op, *out_grads))\r\n    732               else:\r\n    733                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    401       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    402     except ValueError:\r\n--> 403       return grad_fn()  # Exit early\r\n    404 \r\n    405   if not xla_compile:\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    729                 # functions.\r\n    730                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 731                                          lambda: grad_fn(op, *out_grads))\r\n    732               else:\r\n    733                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in custom_grad(self, op, *grads)\r\n    365         \"\"\"\r\n    366         type_name = op.type[5:] if op.type.startswith(\"shap_\") else op.type\r\n--> 367         out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\r\n    368         return out\r\n    369 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in softmax(explainer, op, *grads)\r\n    480                 explainer.between_tensors[t.name] = False\r\n    481 \r\n--> 482     out = tf.gradients(div, in0_centered, grad_ys=grads[0])[0]\r\n    483 \r\n    484     # remove the names we just added\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    729                 # functions.\r\n    730                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 731                                          lambda: grad_fn(op, *out_grads))\r\n    732               else:\r\n    733                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    401       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    402     except ValueError:\r\n--> 403       return grad_fn()  # Exit early\r\n    404 \r\n    405   if not xla_compile:\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    729                 # functions.\r\n    730                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 731                                          lambda: grad_fn(op, *out_grads))\r\n    732               else:\r\n    733                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in custom_grad(self, op, *grads)\r\n    365         \"\"\"\r\n    366         type_name = op.type[5:] if op.type.startswith(\"shap_\") else op.type\r\n--> 367         out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\r\n    368         return out\r\n    369 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in handler(explainer, op, *grads)\r\n    555             return linearity_1d_handler(input_ind1, explainer, op, *grads)\r\n    556         elif var[input_ind0] and var[input_ind1]:\r\n--> 557             return nonlinearity_2d_handler(input_ind0, input_ind1, op_func, explainer, op, *grads)\r\n    558         else:\r\n    559             return [None for _ in op.inputs] # no inputs vary, we must be hidden by a switch function\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/shap/explainers/deep/deep_tf.py in nonlinearity_2d_handler(input_ind0, input_ind1, op_func, explainer, op, *grads)\r\n    626     # Avoid divide by zero nans\r\n    627     out0 = tf.where(tf.abs(tf.tile(delta_in0, dup0)) < 1e-7, tf.zeros_like(out0), out0)\r\n--> 628     out1 = tf.where(tf.abs(tf.tile(delta_in1, dup0)) < 1e-7, tf.zeros_like(out1), out1)\r\n    629 \r\n    630     # see if due to broadcasting our gradient shapes don't match our input shapes\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in where(condition, x, y, name)\r\n   3268       return gen_array_ops.where(condition=condition, name=name)\r\n   3269   elif x is not None and y is not None:\r\n-> 3270     return gen_math_ops.select(condition=condition, x=x, y=y, name=name)\r\n   3271   else:\r\n   3272     raise ValueError(\"x and y must both be non-None or both be None.\")\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py in select(condition, x, y, name)\r\n   9224   # Add nodes to the TensorFlow graph.\r\n   9225   _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 9226         \"Select\", condition=condition, t=x, e=y, name=name)\r\n   9227   _result = _op.outputs[:]\r\n   9228   _inputs_flat = _op.inputs\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    786         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\r\n    787                          input_types=input_types, attrs=attr_protos,\r\n--> 788                          op_def=op_def)\r\n    789       return output_structure, op_def.is_stateful, op\r\n    790 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)\r\n   3614           input_types=input_types,\r\n   3615           original_op=self._default_original_op,\r\n-> 3616           op_def=op_def)\r\n   3617       self._create_op_helper(ret, compute_device=compute_device)\r\n   3618     return ret\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   2025           op_def, inputs, node_def.attr)\r\n   2026       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 2027                                 control_input_ops)\r\n   2028 \r\n   2029     # Initialize self._outputs.\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1865   except errors.InvalidArgumentError as e:\r\n   1866     # Convert to ValueError for backwards compatibility.\r\n-> 1867     raise ValueError(str(e))\r\n   1868 \r\n   1869   return c_op\r\n\r\nValueError: Dimension 1 in both shapes must be equal, but are 10 and 1. Shapes are [?,10] and [?,1]. for 'gradients_1/dense_2/Softmax_grad/gradients/gradients_1/dense_2/Softmax_grad/truediv_grad/Select_1' (op: 'Select') with input shapes: [?,1], [?,10], [?,10].\r\n```\r\n\r\nAny ideas? I'm on TensorFlow 1.14.0, Keras 2.3.1, and shap 0.35.0",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1343/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1343/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
