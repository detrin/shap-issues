{
  "url": "https://api.github.com/repos/shap/shap/issues/2130",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2130/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2130/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2130/events",
  "html_url": "https://github.com/shap/shap/issues/2130",
  "id": 964504178,
  "node_id": "MDU6SXNzdWU5NjQ1MDQxNzg=",
  "number": 2130,
  "title": "Usage for multimodal data (data with fundamentally different characteristics)",
  "user": {
    "login": "trangham283",
    "id": 9788974,
    "node_id": "MDQ6VXNlcjk3ODg5NzQ=",
    "avatar_url": "https://avatars.githubusercontent.com/u/9788974?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/trangham283",
    "html_url": "https://github.com/trangham283",
    "followers_url": "https://api.github.com/users/trangham283/followers",
    "following_url": "https://api.github.com/users/trangham283/following{/other_user}",
    "gists_url": "https://api.github.com/users/trangham283/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/trangham283/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/trangham283/subscriptions",
    "organizations_url": "https://api.github.com/users/trangham283/orgs",
    "repos_url": "https://api.github.com/users/trangham283/repos",
    "events_url": "https://api.github.com/users/trangham283/events{/privacy}",
    "received_events_url": "https://api.github.com/users/trangham283/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2021-08-10T01:04:55Z",
  "updated_at": "2021-08-10T01:12:14Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hello, thank you for a great library. I was wondering if it is possible (or reasonable) to use SHAP for explaining models with mixed (discrete and continuous) inputs. Specifically, my model takes as input a list of tokens (sentence) and a matrix of acoustic features corresponding to that utterance.\r\n\r\nFor the text input, I'm following the example for emotion classification. Specifically:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\nimport shap\r\nimport scipy as sp\r\nfrom datasets import load_dataset\r\nimport torch\r\nfrom torch import nn\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\",use_fast=True)\r\ntext_model = AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\")\r\n\r\ndataset  = load_dataset(\"emotion\", split = \"train\")\r\ndata = pd.DataFrame({'text':dataset['text'],'emotion':dataset['label']})\r\nid2label = text_model.config.id2label\r\nlabel2id = text_model.config.label2id\r\nlabels = sorted(label2id, key=label2id.get)\r\n```\r\n\r\nI have a dummy model that projects acoustic features to the number of classes:\r\n\r\n```\r\nclass_num = 6\r\nspeech_dim = 100\r\nsp_model = nn.Linear(speech_dim, class_num)      \r\n```\r\n\r\nThe overall model is defined as follows; iin this case, I'd also like to use another text-based feature like previous sentence as context:\r\n\r\n```\r\nclass dummyModel(nn.Module):\r\n    def __init__(self, text_model, speech_model=None):\r\n        super(dummyModel, self).__init__()\r\n        self.text_model = text_model\r\n        self.speech_model = speech_model\r\n        \r\n    def forward(self, tvx, tvbg, tvs):\r\n        # tvx: current utterance to classify; tvbg: context utterance; tvs: speech features\r\n        attn_x = (tvx != 0).type(torch.int64)\r\n        attn_bg = (tvbg != 0).type(torch.int64)\r\n\r\n        o1 = self.text_model(tvx, attention_mask=attn_x)[0]\r\n        o2 = self.text_model(tvbg, attention_mask=attn_bg)[0]\r\n        outcomb = o1 + o2\r\n        if self.speech_model is not None:\r\n            o_speech = self.speech_model(tvs)\r\n            outcomb = outcomb + o_speech\r\n        return outcomb\r\n```\r\n\r\nToy data & model:\r\n\r\n```\r\nx_text = data['text'][0:10]\r\nbg_text = data['text'][10:20]\r\ntvs = torch.rand((10, speech_dim))\r\n\r\ntvx = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128,truncation=True) for v in x_text])\r\ntvbg = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128,truncation=True) for v in bg_text])\r\n\r\nattn_x = (tvx != 0).type(torch.int64)\r\nattn_bg = (tvbg != 0).type(torch.int64)\r\n\r\nbatch = [tvx, tvbg, tvs]\r\n\r\nmy_model = dummyModel(text_model)\r\n```\r\n\r\nIdeally, I'd like to get an explainer: \r\n\r\n```\r\ne = shap.Explainer(my_model, ...)\r\n```\r\n\r\nHowever, from my understanding, explaining text inputs at the token level uses the PartitionExplainer, while a use case like this (multiple inputs) maybe GradientExplainer is more appropriate ([from this example](https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/gradient_explainer/Multi-input%20Gradient%20Explainer%20MNIST%20Example.html)).\r\n\r\nI think the issue is that the tokens are masked using the tokenizer, where essentially sampling of missingness is based on using the [MASK] token (and in the embedding space the whole vector is masked). On the other hand, to explain other (e.g. acoustic features), each dimension is considered independently. \r\n\r\nIs there a way to get around this? Any pointers would be appreciated, thank you very much! \r\n\r\n\r\n\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2130/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2130/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
