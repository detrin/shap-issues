{
  "url": "https://api.github.com/repos/shap/shap/issues/1289",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1289/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1289/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1289/events",
  "html_url": "https://github.com/shap/shap/issues/1289",
  "id": 644913538,
  "node_id": "MDU6SXNzdWU2NDQ5MTM1Mzg=",
  "number": 1289,
  "title": "Multioutput chained regression using TreeExplainer",
  "user": {
    "login": "astrogilda",
    "id": 23521054,
    "node_id": "MDQ6VXNlcjIzNTIxMDU0",
    "avatar_url": "https://avatars.githubusercontent.com/u/23521054?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/astrogilda",
    "html_url": "https://github.com/astrogilda",
    "followers_url": "https://api.github.com/users/astrogilda/followers",
    "following_url": "https://api.github.com/users/astrogilda/following{/other_user}",
    "gists_url": "https://api.github.com/users/astrogilda/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/astrogilda/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/astrogilda/subscriptions",
    "organizations_url": "https://api.github.com/users/astrogilda/orgs",
    "repos_url": "https://api.github.com/users/astrogilda/repos",
    "events_url": "https://api.github.com/users/astrogilda/events{/privacy}",
    "received_events_url": "https://api.github.com/users/astrogilda/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2020-06-24T20:12:35Z",
  "updated_at": "2020-06-24T20:33:01Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hello,\r\n\r\nMy problem calls from prediction of two output labels. While I can do this by creating two independent models, my outputs are co-dependent, and I'd like to use the `RegressorChain` wrapper from sklearn--it uses the union of the prediction of the 1st model and the input features to predict and output of the 2nd model.\r\n\r\nWhen I plot feature importances (summary plot) for the second label, a few of the top features are ones which shouldn't impact it (based on domain knowledge), but pop up because they are important for the first label and the first and second labels are co-dependent. I would like to isolate the impact of only those features which are directly important for predicting the second feature. Is the following a sensible way to go about doing this:\r\n(i) Use a model to predict label1, and use summary plot to get feature importances for this label.\r\n(ii) Use input features + pred_label1 to predict label 2, and get shap values using TreeExplainer, but don't use summary plot. Manually eliminate the shapley values corresponding to pred_label1, and THEN use summary plot. Do the same thing if interaction values are needed.\r\n\r\nThoughts?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1289/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1289/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
