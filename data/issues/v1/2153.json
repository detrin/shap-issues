{
  "url": "https://api.github.com/repos/shap/shap/issues/2153",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2153/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2153/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2153/events",
  "html_url": "https://github.com/shap/shap/issues/2153",
  "id": 979376610,
  "node_id": "MDU6SXNzdWU5NzkzNzY2MTA=",
  "number": 2153,
  "title": "SHAP Values for Network with LSTM and Dense Layer Input Branches ",
  "user": {
    "login": "hp2500",
    "id": 27285925,
    "node_id": "MDQ6VXNlcjI3Mjg1OTI1",
    "avatar_url": "https://avatars.githubusercontent.com/u/27285925?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/hp2500",
    "html_url": "https://github.com/hp2500",
    "followers_url": "https://api.github.com/users/hp2500/followers",
    "following_url": "https://api.github.com/users/hp2500/following{/other_user}",
    "gists_url": "https://api.github.com/users/hp2500/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/hp2500/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/hp2500/subscriptions",
    "organizations_url": "https://api.github.com/users/hp2500/orgs",
    "repos_url": "https://api.github.com/users/hp2500/repos",
    "events_url": "https://api.github.com/users/hp2500/events{/privacy}",
    "received_events_url": "https://api.github.com/users/hp2500/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-08-25T16:05:04Z",
  "updated_at": "2022-11-08T15:23:57Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi there, \r\n\r\nthank you for the excellent work! I am trying to generate SHAP values for a model with two input branches: One LSTM branch that ingests sequential data (3D array) and one that ingests non-sequential data (2D array). \r\n\r\nThe model builder looks like this:\r\n\r\n```python\r\ndef get_compiled_model(X_seq_shape, X_mom_shape):\r\n    \r\n    # define two sets of inputs\r\n    input_sequence = Input(shape=(X_seq_shape[1], \r\n                                  X_seq_shape[2]))\r\n\r\n    input_other = Input(shape=(X_mom_shape[1]))\r\n\r\n    # firBatchNormalizationanch uses sequence data\r\n    x_seq = LSTM(units=128, return_sequences=True,\r\n                 dropout=0.2,\r\n                 recurrent_dropout=0)(input_sequence)\r\n    x_seq = LSTM(units=64, return_sequences=True,\r\n                 dropout=0.2,\r\n                 recurrent_dropout=0)(x_seq)\r\n    x_seq = LSTM(units=64, return_sequences=True,\r\n                 dropout=0.2,\r\n                 recurrent_dropout=0)(x_seq)\r\n    x_seq = LSTM(units=32, return_sequences=True,\r\n                 dropout=0.2,\r\n                 recurrent_dropout=0)(x_seq)\r\n    x_seq = LSTM(units=32)(x_seq)\r\n    x_seq = Model(inputs=input_sequence, outputs=x_seq)\r\n\r\n    # second branch uses momentary data\r\n    x_oth = Dense(128, activation=\"relu\")(input_other)\r\n    x_oth = Dropout(0.2)(x_oth)\r\n    x_oth = Dense(64, activation=\"relu\")(x_oth)\r\n    x_oth = Dropout(0.2)(x_oth)\r\n    x_oth = Dense(64, activation=\"relu\")(x_oth)\r\n    x_oth = Dropout(0.2)(x_oth)\r\n    x_oth = Dense(32, activation=\"relu\")(x_oth)\r\n    x_oth = Dropout(0.2)(x_oth)\r\n    x_oth = Dense(32, activation=\"relu\")(x_oth)\r\n    x_oth = Model(inputs=input_other, outputs=x_oth)\r\n\r\n    # concat output of the two branches\r\n    combined = Concatenate()([x_seq.output, x_oth.output])\r\n    x_comb = Dense(8, activation=\"relu\")(combined)\r\n    x_comb = Dense(1, activation=\"linear\")(x_comb)\r\n\r\n    model = Model(inputs=[x_seq.input, x_oth.input], outputs=x_comb)\r\n    \r\n    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\r\n\r\n    return model\r\n```\r\n\r\nTraining the model works well:\r\n\r\n```python\r\n\r\nmodel = get_compiled_model(X_seq_train.shape, X_mom_train.shape)\r\n    \r\nmodel.fit(x=[X_seq_train, X_mom_train], y=target_train,\r\n          validation_data = ([X_seq_val, X_mom_val], target_val),\r\n          epochs=100, batch_size=1024, shuffle=True)\r\n```\r\n\r\nNow, when I try to generate SHAP values I am getting an error related to the dimensionality of the two input arrays: \r\n\r\n```python\r\nsample = np.random.choice(X_seq_train.shape[0], 100, replace=False)\r\nbackground = [np.take(X_seq_train, sample, axis=0),\r\n              np.take(X_mom_train, sample, axis=0)]\r\n\r\nex = shap.DeepExplainer(model, background)\r\n\r\nshap_values = ex.shap_values([X_seq_test[0:10,:,:], X_mom_test[10,:]])\r\n```\r\n\r\nThe last line is throwing the error: \r\n\r\n```python\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/tmp/ipykernel_26501/3289302125.py in <module>\r\n----> 1 shap_values = ex.shap_values([X_seq_test[0:10,:,X_seq_idx], X_mom_test[0:10,X_mom_idx]])\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/__init__.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    122             were chosen as \"top\".\r\n    123         \"\"\"\r\n--> 124         return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\r\n    306                 # run attribution computation graph\r\n    307                 feature_ind = model_output_ranks[j,i]\r\n--> 308                 sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)\r\n    309 \r\n    310                 # assign the attributions to the right part of the output arrays\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in run(self, out, model_inputs, X)\r\n    363 \r\n    364                 return final_out\r\n--> 365             return self.execute_with_overridden_gradients(anon)\r\n    366 \r\n    367     def custom_grad(self, op, *grads):\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in execute_with_overridden_gradients(self, f)\r\n    399         # define the computation graph for the attribution values using a custom gradient-like computation\r\n    400         try:\r\n--> 401             out = f()\r\n    402         finally:\r\n    403             # reinstate the backpropagatable check\r\n\r\n/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py in anon()\r\n    359                     v = tf.constant(data, dtype=self.model_inputs[i].dtype)\r\n    360                     inputs.append(v)\r\n--> 361                 final_out = out(inputs)\r\n    362                 tf_execute.record_gradient = tf_backprop._record_gradient\r\n    363 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    812       # In this case we have not created variables on the first call. So we can\r\n    813       # run the first trace but we should fail if variables are created.\r\n--> 814       results = self._stateful_fn(*args, **kwds)\r\n    815       if self._created_variables:\r\n    816         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2826     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   2827     with self._lock:\r\n-> 2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:243 grad_graph  *\r\n        out = self.model(shap_rAnD)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__  **\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\r\n        inputs, training=training, mask=mask)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\r\n        outputs = node.layer(*args, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:659 __call__\r\n        return super(RNN, self).__call__(inputs, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:1183 call\r\n        runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:1559 lstm_with_backend_selection\r\n        function.register(defun_gpu_lstm, **params)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:3241 register\r\n        concrete_func.add_gradient_functions_to_graph()\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:2063 add_gradient_functions_to_graph\r\n        self._delayed_rewrite_functions.forward_backward())\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:621 forward_backward\r\n        forward, backward = self._construct_forward_backward(num_doutputs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward\r\n        func_graph=backwards_graph)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py:659 _backprop_function\r\n        src_graph=self._func_graph)\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:669 _GradientsHelper\r\n        lambda: grad_fn(op, *out_grads))\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:336 _MaybeCompile\r\n        return grad_fn()  # Exit early\r\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:669 <lambda>\r\n        lambda: grad_fn(op, *out_grads))\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:371 custom_grad\r\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:660 handler\r\n        return linearity_with_excluded_handler(input_inds, explainer, op, *grads)\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:667 linearity_with_excluded_handler\r\n        assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\r\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:220 _variable_inputs\r\n        out[i] = t.name in self.between_tensors\r\n\r\n    AttributeError: 'TFDeep' object has no attribute 'between_tensors'\r\n```\r\n\r\nI am not sure if multi branch network architectures like the one I am using are currently supported. If this is not the case, would you be so kind an point me to a workaround for this problem? ",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2153/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2153/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
