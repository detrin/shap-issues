{
  "url": "https://api.github.com/repos/shap/shap/issues/1731",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1731/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1731/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1731/events",
  "html_url": "https://github.com/shap/shap/issues/1731",
  "id": 784212719,
  "node_id": "MDU6SXNzdWU3ODQyMTI3MTk=",
  "number": 1731,
  "title": "Shapley value (or SHAP) and the correlation between input features",
  "user": {
    "login": "xushanthu-2014",
    "id": 60925333,
    "node_id": "MDQ6VXNlcjYwOTI1MzMz",
    "avatar_url": "https://avatars.githubusercontent.com/u/60925333?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/xushanthu-2014",
    "html_url": "https://github.com/xushanthu-2014",
    "followers_url": "https://api.github.com/users/xushanthu-2014/followers",
    "following_url": "https://api.github.com/users/xushanthu-2014/following{/other_user}",
    "gists_url": "https://api.github.com/users/xushanthu-2014/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/xushanthu-2014/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/xushanthu-2014/subscriptions",
    "organizations_url": "https://api.github.com/users/xushanthu-2014/orgs",
    "repos_url": "https://api.github.com/users/xushanthu-2014/repos",
    "events_url": "https://api.github.com/users/xushanthu-2014/events{/privacy}",
    "received_events_url": "https://api.github.com/users/xushanthu-2014/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2021-01-12T12:58:33Z",
  "updated_at": "2021-01-12T12:58:33Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi\r\nThanks for your work of SHAP! And it's a really nice tool to interpret the deep learning/machine learning model.\r\nCurrently I am using SHAP to interpret the DNN trained on data in my field. We are using time series data in remote sensing to train DNNs. But I got questions about SHAP values: \r\nQ1. will the calculation of SHAP be affected by the correlation of input features? \r\nQ2. and what's the difference between SHAP values and sensitivity (by sensitivity I mean give a perturbation of input variables individually then calculate the ratio of perburbed prediction to the perturbed input variables)?\r\n\r\nBackground for Q1: Regarding to the first question, we are using samples from time series data. Suppose we have daily data of during 2007 to 2020 year, then we got the number of sample as 14*365. And assume here we pay no attention to the memory effect of time series. Then we are using different features as input variables (say the number of feature is 16, X1,...,X16) to simulate one other variable Y. That means we have a pair of [X1,...,X16, Y] in every day. In that case our input matrix is of shape [16, 14*365] and output data is of shape [1, 14*365] (label Y has been normalised before training). And by using DeepExplainer we calculated the SHAP values. As a result we are able to get feature importance as a matrix of [16, 14*365]. But here comes the issue: if there is some correlation between Xi and Xj, then we will get a weird plot in which the patterns of Xi and Xj are opposite to each other and their sum is close to zero. The figure is attached:\r\n![image](https://user-images.githubusercontent.com/60925333/104316791-478f2080-54dd-11eb-9a08-cb79ca4997bc.png)\r\n\r\nThe x-axis is the time, day of year 0-365 during one year (indicating different samples), and y-axis is the SHAP values of Xi and Xj. Suppose Xi is the green dash line and Xj is the blue dash line (just ignore the blue dot line). \r\n\r\nFrom meteorology I know that Xi and Xj are somehow correlated (positively but not negatively correlated). So from this figure can we state that SHAP value (or our model) just jumps between Xi and Xj? at some time points (for some samples), SHAP concentrates on Xi while at another time points (or some samples) SHAP concentrates on Xj? But the sum of SHAP values of Xi and Xj are close to zero which means the total contribution of Xi and Xj to the [f(x) - Ef(x)] is little....so how can I fix this issue?\r\n\r\nThanks and hope for your early reply!\r\n\r\nSincerely\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1731/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1731/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
