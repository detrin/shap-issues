{
  "url": "https://api.github.com/repos/shap/shap/issues/2849",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2849/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2849/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2849/events",
  "html_url": "https://github.com/shap/shap/issues/2849",
  "id": 1566030931,
  "node_id": "I_kwDOBHDcK85dV7xT",
  "number": 2849,
  "title": "Text masker issue with deberta tokenizer and other bpe tokenizers",
  "user": {
    "login": "weihaosong",
    "id": 20724447,
    "node_id": "MDQ6VXNlcjIwNzI0NDQ3",
    "avatar_url": "https://avatars.githubusercontent.com/u/20724447?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/weihaosong",
    "html_url": "https://github.com/weihaosong",
    "followers_url": "https://api.github.com/users/weihaosong/followers",
    "following_url": "https://api.github.com/users/weihaosong/following{/other_user}",
    "gists_url": "https://api.github.com/users/weihaosong/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/weihaosong/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/weihaosong/subscriptions",
    "organizations_url": "https://api.github.com/users/weihaosong/orgs",
    "repos_url": "https://api.github.com/users/weihaosong/repos",
    "events_url": "https://api.github.com/users/weihaosong/events{/privacy}",
    "received_events_url": "https://api.github.com/users/weihaosong/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2023-02-01T12:30:56Z",
  "updated_at": "2023-02-01T12:30:56Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hello, I encountered some interesting behavior of the text masker class when applying deberta tokenizer (bpe tokenizers).\r\n\r\nSpecifically, this line in [text masker](https://github.com/slundberg/shap/blob/master/shap/maskers/_text.py#L122) loops over the mask and reconstructs masked text as output. \r\n\r\nIt relies on this function [token_segments](https://github.com/slundberg/shap/blob/master/shap/maskers/_text.py#L165) to get the substring associated with each token using offsets from the tokenizer. However, this assumes the offsets returned from tokenizer do not contain duplicates. Otherwise, the masked text could have duplicate substrings.\r\n\r\nThis duplication could happen when deberta tokenizer (maybe other bpe tokenizers such as GPT-2) return two tokens for a single character.\r\nBelow is an example:\r\n\r\n\r\n<img width=\"1173\" alt=\"Screen Shot 2023-02-01 at 12 18 42 PM\" src=\"https://user-images.githubusercontent.com/20724447/216041592-2b2988ed-2f74-499a-bf89-0fc94665b3e4.png\">\r\n\r\nThe end result is we duplicated the substring that maps to two tokens in the output and this will cause difference in model prediction and shap value sum because input is not the same.\r\n\r\n<img width=\"553\" alt=\"Screen Shot 2023-02-01 at 12 23 37 PM\" src=\"https://user-images.githubusercontent.com/20724447/216041954-f894369f-d920-458a-bb52-27bf864d0c8e.png\">\r\n\r\nMy current way of dealing this is to filter out single characters that map to two tokens but I don't have an exhaustive list of all these characters. Just wondering if there is there a better way to do this? Thanks a lot!\r\n\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2849/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2849/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
