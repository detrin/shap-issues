{
  "url": "https://api.github.com/repos/shap/shap/issues/1776",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1776/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1776/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1776/events",
  "html_url": "https://github.com/shap/shap/issues/1776",
  "id": 792721795,
  "node_id": "MDU6SXNzdWU3OTI3MjE3OTU=",
  "number": 1776,
  "title": "How to use TreeExplainer on XGBoost model trained with Sparse CSR matrix for model_output='probability'",
  "user": {
    "login": "eaishwa",
    "id": 44345732,
    "node_id": "MDQ6VXNlcjQ0MzQ1NzMy",
    "avatar_url": "https://avatars.githubusercontent.com/u/44345732?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/eaishwa",
    "html_url": "https://github.com/eaishwa",
    "followers_url": "https://api.github.com/users/eaishwa/followers",
    "following_url": "https://api.github.com/users/eaishwa/following{/other_user}",
    "gists_url": "https://api.github.com/users/eaishwa/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/eaishwa/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/eaishwa/subscriptions",
    "organizations_url": "https://api.github.com/users/eaishwa/orgs",
    "repos_url": "https://api.github.com/users/eaishwa/repos",
    "events_url": "https://api.github.com/users/eaishwa/events{/privacy}",
    "received_events_url": "https://api.github.com/users/eaishwa/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-01-24T05:03:02Z",
  "updated_at": "2022-11-24T13:31:07Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "@slundberg Thanks for this amazing library. I have a model object of type xgboost.sklearn.XGBClassifier, which was trained with data passed from column transformer in sklearn. Hence it was a sparse CSR matrix that was used in sklearn pipeline to train the model. I want to use SHAP TreeExplainer to explain output probability. TreeExplainer accepts only np array or pandas df as input for the background data. So I am unable to pass the transformed input data to it. I get the following error if I use sparse matrix -\r\n\r\n```\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in __init__(self, model, data,\r\n\r\nmodel_output, feature_perturbation, feature_names, **deprecated_options)\r\n    145         self.feature_perturbation = feature_perturbation\r\n    146         self.expected_value = None\r\n--> 147         self.model = TreeEnsemble(model, self.data, self.data_missing, model_output)\r\n    148         self.model_output = model_output\r\n    149         #self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\r\n\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in __init__(self, model, data, data_missing, model_output)\r\n    825             self.original_model = model.get_booster()\r\n    826             xgb_loader = XGBTreeModelLoader(self.original_model)\r\n--> 827             self.trees = xgb_loader.get_trees(data=data, data_missing=data_missing)\r\n    828             self.base_offset = xgb_loader.base_score\r\n    829             self.objective = objective_name_map.get(xgb_loader.name_obj, None)\r\n\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in get_trees(self, data, data_missing)\r\n   1528                 \"value\": self.values[i,:l],\r\n   1529                 \"node_sample_weight\": self.sum_hess[i]\r\n-> 1530             }, data=data, data_missing=data_missing))\r\n   1531         return trees\r\n   1532 \r\n\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in __init__(self, tree, normalize, scaling, data, data_missing)\r\n   1353             _cext.dense_tree_update_weights(\r\n   1354                 self.children_left, self.children_right, self.children_default, self.features,\r\n-> 1355                 self.thresholds, self.values, 1, self.node_sample_weight, data, data_missing\r\n   1356             )\r\n   1357 \r\n\r\nSystemError: <built-in function dense_tree_update_weights> returned NULL without setting an error\r\n\r\n```\r\nThe code I used is - \r\n`explainer_prob = shap.TreeExplainer(xgb_fit['xgbclassifier']\r\n                               ,feature_perturbation ='interventional'\r\n                               ,model_output='probability'\r\n                               ,data=shap.sample(x_train_samp,100))`\r\n\r\nIf I passed dense format background data to the above explainer, it runs but then generating shap_values fails on the sparse csr matrix with the following error - \r\n\r\n```\r\n----> 8 shap_values_prob = explainer_prob.shap_values(trans)\r\n      9 np.abs(shap_values_prob.sum(1) + explainer_prob.expected_value - pred_prob[:,1]).max()\r\n\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in shap_values(self, X, y, tree_limit, approximate, check_additivity, from_call)\r\n    380         X, y, X_missing, flat_output, tree_limit, check_additivity = self._validate_inputs(X, y,\r\n    381                                                                                            tree_limit,\r\n--> 382                                                                                            check_additivity)\r\n    383         transform = self.model.get_transform()\r\n    384 \r\n\r\n~/anaconda3/envs/python3/lib/python3.6/site-packages/shap/explainers/_tree.py in _validate_inputs(self, X, y, tree_limit, check_additivity)\r\n    245         if X.dtype != self.model.input_dtype:\r\n    246             X = X.astype(self.model.input_dtype)\r\n--> 247         X_missing = np.isnan(X, dtype=np.bool)\r\n    248         assert isinstance(X, np.ndarray), \"Unknown instance type: \" + str(type(X))\r\n    249         assert len(X.shape) == 2, \"Passed input data matrix X must have 1 or 2 dimensions!\"\r\n\r\nTypeError: No loop matching the specified signature and casting was found for ufunc isnan\r\n```\r\nI can't pass dense array to get shap values because XGBoost gives different predictions with dense array. As [ the doc](https://xgboost.readthedocs.io/en/latest/faq.html#why-do-i-see-different-results-with-sparse-and-dense-data) suggests, I should use the same data format for training and testing and I want to stick to sparse data because I use columntransformer in sklearn pipeline while training. How can I use TreeExplainer with model_output='probability' in this scenario. Thanks in advance.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1776/reactions",
    "total_count": 5,
    "+1": 5,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1776/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
