{
  "url": "https://api.github.com/repos/shap/shap/issues/2331",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2331/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2331/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2331/events",
  "html_url": "https://github.com/shap/shap/issues/2331",
  "id": 1084916378,
  "node_id": "I_kwDOBHDcK85AqoKa",
  "number": 2331,
  "title": "Applying SHAP with custom tokenizer to BERT",
  "user": {
    "login": "FiammettaC",
    "id": 26167801,
    "node_id": "MDQ6VXNlcjI2MTY3ODAx",
    "avatar_url": "https://avatars.githubusercontent.com/u/26167801?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/FiammettaC",
    "html_url": "https://github.com/FiammettaC",
    "followers_url": "https://api.github.com/users/FiammettaC/followers",
    "following_url": "https://api.github.com/users/FiammettaC/following{/other_user}",
    "gists_url": "https://api.github.com/users/FiammettaC/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/FiammettaC/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/FiammettaC/subscriptions",
    "organizations_url": "https://api.github.com/users/FiammettaC/orgs",
    "repos_url": "https://api.github.com/users/FiammettaC/repos",
    "events_url": "https://api.github.com/users/FiammettaC/events{/privacy}",
    "received_events_url": "https://api.github.com/users/FiammettaC/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-12-20T15:25:05Z",
  "updated_at": "2022-06-30T07:33:42Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Is it correct to use SHAP + a masker with a custom tokenizer to explain a fine-tuned BERT model?\r\n\r\nMy doubt is linked to the different tokenization, since BERT tokenizer does WordPiece tokenization (e.g. 'lovely' becomes 'love', '##ly'), but with SHAP I would like to have token level explanations, so using a is a simple white space tokenizer.\r\n\r\nWithout the masker, I obtain this result:\r\n<img width=\"491\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26167801/146790543-c6f3603b-bad9-4dba-81ba-c0fa43e1d5de.png\">\r\n\r\n\r\nWhile, with the masker, I get the result that I want (lovely is one word, not split according to the BERT tokenization.\r\nHere is an extract from the code, with masker and explainer:\r\n`masker = shap.maskers.Text(r\"\\s\")`\r\n `explainer = shap.Explainer(f, masker)`\r\n `shap_values = explainer(sentences)`\r\n\r\nf is the prediction function, which takes the fine-tuned BERT model and the pre-trained BERT tokenizer as input.\r\n\r\nBut is this approach correct? \r\nCan I use a white space tokenizer, although my model is trained differently?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2331/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2331/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
