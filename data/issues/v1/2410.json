{
  "url": "https://api.github.com/repos/shap/shap/issues/2410",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2410/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2410/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2410/events",
  "html_url": "https://github.com/shap/shap/issues/2410",
  "id": 1148035980,
  "node_id": "I_kwDOBHDcK85EbaOM",
  "number": 2410,
  "title": "Performance difference KernelExplainer model vs. function",
  "user": {
    "login": "akarpf",
    "id": 34109275,
    "node_id": "MDQ6VXNlcjM0MTA5Mjc1",
    "avatar_url": "https://avatars.githubusercontent.com/u/34109275?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/akarpf",
    "html_url": "https://github.com/akarpf",
    "followers_url": "https://api.github.com/users/akarpf/followers",
    "following_url": "https://api.github.com/users/akarpf/following{/other_user}",
    "gists_url": "https://api.github.com/users/akarpf/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/akarpf/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/akarpf/subscriptions",
    "organizations_url": "https://api.github.com/users/akarpf/orgs",
    "repos_url": "https://api.github.com/users/akarpf/repos",
    "events_url": "https://api.github.com/users/akarpf/events{/privacy}",
    "received_events_url": "https://api.github.com/users/akarpf/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2022-02-23T12:33:55Z",
  "updated_at": "2022-02-26T19:53:13Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi,\r\n\r\nI am trying to reproduce/speed up the code of the following paper, which uses SHAP: https://arxiv.org/pdf/1903.02407.pdf (https://github.com/ronniemi/explainAnomaliesUsingSHAP).\r\n\r\nI am noticing a huge difference in performance when I use KernelExplainer with a function or a model respectively. This is puzzling me.\r\n\r\nIn the paper the authors runs the KernelExplainer per feature:\r\n\r\n```\r\n#select feature\r\ncounter = df_top_err.index[0]\r\n```\r\n```\r\ndef func_predict_feature(record):\r\n    record_prediction = autoencoder.predict(record)[:,counter]\r\n    return record_prediction\r\n\r\nexplainer = shap.KernelExplainer(func_predict_feature, background_set)\r\nshap_values2 = explainer.shap_values(record_to_explain, nsamples='auto')\r\n\r\n# --- 10.933740139007568 seconds ---\r\n```\r\n```\r\nprint(shap_values)\r\n[ 0.          0.          0.          0.          0.          0.\r\n -0.00014713  0.          0.          0.          0.          0.\r\n  0.          0.          0.          0.          0.          0.\r\n -0.00012618  0.          0.          0.          0.          0.\r\n  0.          0.          0.          0.          0.        ]\r\n```\r\n\r\nIn the process of experimenting, trying to speed things up, I came up with the following solution, which in my opinion should be equivalent.:\r\n\r\n```\r\nmodelsub = Model(autoencoder.layers[0].input, autoencoder.layers[-1].output[:,counter])\r\n\r\nexplainer = shap.KernelExplainer(modelsub, background_set)\r\nshap_values = explainer.shap_values(record_to_explain, nsamples='auto')\r\n\r\n# --- 0.8240640163421631 seconds ---\r\n```\r\n```\r\nprint(shap_values)\r\n[ 0.          0.          0.          0.          0.          0.\r\n -0.00015826  0.          0.          0.          0.          0.\r\n  0.          0.          0.          0.          0.          0.\r\n -0.00011506  0.          0.          0.          0.          0.\r\n  0.          0.          0.          0.          0.        ]\r\n```\r\n\r\nThe results are basically identical up to 4 places behind the decimal point. But the second variant is >10x faster. How is that possible? Or am I missing something here?\r\n\r\nAs I am having your attention already, I might as well ask you another question: Would this be the corresponding solution with DeepExplainer (using the whole data as a background set)?\r\n\r\n```\r\nmodelsub = Model(autoencoder.layers[0].input, autoencoder.layers[-1].output[:,counter])\r\nexplainer = shap.DeepExplainer(modelsub, x_train.values)\r\nshap_values = explainer.shap_values(record_to_explain.values[np.newaxis],check_additivity=False)\r\n\r\n#--- 0.5212132930755615 seconds ---\r\n```\r\n\r\nPlease note that I had to reshape the data `record_to_explain.values[np.newaxis]` and set `check_additivity=False` in order to get DeepExplainer working (might this be the problem?). It's even faster, but gives vastly different results: \r\n\r\n```\r\nprint(shap_values)\r\n[ 0.     -0.0001  0.0001  0.0003 -0.      0.0001 -0.0004 -0.     -0.\r\n   0.0001  0.      0.0001 -0.      0.     -0.0001 -0.0001  0.     -0.\r\n  -0.0002 -0.     -0.      0.     -0.     -0.      0.     -0.0001  0.0001\r\n  -0.      0.0002]\r\n```\r\n\r\nI am aware that this is not Stackoverflow, but I saw that other people here had discussed the slow performance of KernelExplainer before. The only suggestions I got from these other threads however were to reduce the size of the background set (reducing the the quality of shapley values) or to use DeepExplainer.\r\n\r\nI would highly appreciate if you could comment on my question. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n ",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2410/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2410/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
