{
  "url": "https://api.github.com/repos/shap/shap/issues/2397",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2397/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2397/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2397/events",
  "html_url": "https://github.com/shap/shap/issues/2397",
  "id": 1143354404,
  "node_id": "I_kwDOBHDcK85EJjQk",
  "number": 2397,
  "title": "Add support for spacy models",
  "user": {
    "login": "ycouble",
    "id": 4104153,
    "node_id": "MDQ6VXNlcjQxMDQxNTM=",
    "avatar_url": "https://avatars.githubusercontent.com/u/4104153?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ycouble",
    "html_url": "https://github.com/ycouble",
    "followers_url": "https://api.github.com/users/ycouble/followers",
    "following_url": "https://api.github.com/users/ycouble/following{/other_user}",
    "gists_url": "https://api.github.com/users/ycouble/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ycouble/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ycouble/subscriptions",
    "organizations_url": "https://api.github.com/users/ycouble/orgs",
    "repos_url": "https://api.github.com/users/ycouble/repos",
    "events_url": "https://api.github.com/users/ycouble/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ycouble/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2022-02-18T16:24:32Z",
  "updated_at": "2022-06-30T09:52:30Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "Hi,\r\n\r\nRecently I tried to use the text explainer, similarly to what's presented in your example with the transformers.\r\n\r\nI managed to hack a solution to make it \"work\" by adding some wrappers around the spacy tokenizer / model. The results are sensible, but I don't know to what extent. It is quite hacky and I had to do a lot of retro-engineering of shap to get to this level.\r\n\r\nAre you considering adding support for spacy like you did for the transformers library (I think the tokenizer is the main difficulty) ? If not, are there any reason not to do it ?\r\n\r\nBest,\r\nYoann\r\n\r\n---\r\n\r\nFor reference, here is my \"hacky\" code to be able to use shap with a spacy model.\r\n\r\n- Wrappers around my spacy model for prediction / tokenization:\r\n```python\r\nimport spacy\r\ntextcat_spacy = spacy.load(\"my-model\")\r\ntokenizer_spacy = spacy.tokenizer.Tokenizer(textcat_spacy.vocab)\r\n\r\n# Run the spacy pipeline on some random text just to retrieve the classes\r\ndoc = textcat_spacy(\"hi\")\r\nclasses = list(doc.cats.keys())\r\n\r\n# Define a function to predict\r\ndef predict(texts):\r\n    # convert texts to bare strings\r\n    texts = [str(text) for text in texts]\r\n    results = []\r\n    for doc in textcat_spacy.pipe(texts):\r\n        # results.append([{'label': cat, 'score': doc.cats[cat]} for cat in doc.cats])\r\n        results.append([doc.cats[cat] for cat in classes])\r\n    return results\r\n\r\n# Create a function to create a transformers-like tokenizer to match shap's expectations\r\ndef tok_adapter(text, return_offsets_mapping=False):\r\n    doc = tokenizer_spacy(text)\r\n    out = {\"input_ids\": [tok.norm for tok in doc]}\r\n    if return_offsets_mapping:\r\n        out[\"offset_mapping\"] = [(tok.idx, tok.idx + len(tok)) for tok in doc]\r\n    return out\r\n```\r\n\r\n- The Shap Explainer configuration:\r\n```python\r\nimport shap\r\n# Create the Shap Explainer\r\n# - predict is the \"model\" function, adapted to a transformers-like model\r\n# - masker is the masker used by shap, which relies on a transformers-like tokenizer\r\n# - algorithm is set to permuation, which is the one used for transformers models\r\n# - output_names are the classes (altough it is not propagated to the permutation explainer currently, which is why plots do not have the labels)\r\n# - max_evals is set to a high number to reduce the probability of cases where the explainer fails because there are too many tokens\r\nexplainer = shap.Explainer(predict, masker=shap.maskers.Text(tok_adapter), algorithm=\"permutation\", output_names=classes, max_evals=1500)\r\n```\r\n\r\n- Usage:\r\n```python\r\nsample = \"Some text to classify\"\r\n# Process the text using SpaCy\r\ndoc = textcat_spacy(sample)\r\n# Get the shap values\r\nshap_values = explainer([sample])\r\n```\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2397/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2397/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
