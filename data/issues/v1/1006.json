{
  "url": "https://api.github.com/repos/shap/shap/issues/1006",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1006/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1006/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1006/events",
  "html_url": "https://github.com/shap/shap/issues/1006",
  "id": 552366124,
  "node_id": "MDU6SXNzdWU1NTIzNjYxMjQ=",
  "number": 1006,
  "title": "Scaling baseline and SHAPs back to original class rate",
  "user": {
    "login": "givenx",
    "id": 46048150,
    "node_id": "MDQ6VXNlcjQ2MDQ4MTUw",
    "avatar_url": "https://avatars.githubusercontent.com/u/46048150?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/givenx",
    "html_url": "https://github.com/givenx",
    "followers_url": "https://api.github.com/users/givenx/followers",
    "following_url": "https://api.github.com/users/givenx/following{/other_user}",
    "gists_url": "https://api.github.com/users/givenx/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/givenx/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/givenx/subscriptions",
    "organizations_url": "https://api.github.com/users/givenx/orgs",
    "repos_url": "https://api.github.com/users/givenx/repos",
    "events_url": "https://api.github.com/users/givenx/events{/privacy}",
    "received_events_url": "https://api.github.com/users/givenx/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2020-01-20T15:23:25Z",
  "updated_at": "2020-05-07T11:05:33Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I have an imbalanced dataset (positive class rate = 1%) and have downsampled the negative class to give me a 50/50 balance in the two classes.  Ignoring the challenges that comes with undersampling (loss of information), the baseline of the SHAP outputs is now close to 50% as expected.  However, I need to present the SHAP outputs including the baseline value back to the end users so that the resulting SHAP probabilities are intuitive.  Is it correct (I have a feeling it is not) to apply a straight scaling factor of 1/50 to all the SHAP values and the baseline value to bring the SHAPs in line with the original positive class rate...?  I am using lightgbm for the algorithm, not sure if that matters.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1006/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1006/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
