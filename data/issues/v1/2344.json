{
  "url": "https://api.github.com/repos/shap/shap/issues/2344",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2344/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2344/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2344/events",
  "html_url": "https://github.com/shap/shap/issues/2344",
  "id": 1094188826,
  "node_id": "I_kwDOBHDcK85BN_8a",
  "number": 2344,
  "title": "Can SHAP explain a model based on the lack of presence of a particular feature?",
  "user": {
    "login": "NataliaDiaz",
    "id": 2312327,
    "node_id": "MDQ6VXNlcjIzMTIzMjc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/2312327?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/NataliaDiaz",
    "html_url": "https://github.com/NataliaDiaz",
    "followers_url": "https://api.github.com/users/NataliaDiaz/followers",
    "following_url": "https://api.github.com/users/NataliaDiaz/following{/other_user}",
    "gists_url": "https://api.github.com/users/NataliaDiaz/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/NataliaDiaz/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/NataliaDiaz/subscriptions",
    "organizations_url": "https://api.github.com/users/NataliaDiaz/orgs",
    "repos_url": "https://api.github.com/users/NataliaDiaz/repos",
    "events_url": "https://api.github.com/users/NataliaDiaz/events{/privacy}",
    "received_events_url": "https://api.github.com/users/NataliaDiaz/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2022-01-05T10:13:37Z",
  "updated_at": "2022-01-05T10:14:15Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "1. Can SHAP explain a model based on a particular example f(x_i) = y_i and that this is because feature f is NOT present in x_i? It is very important that the model does NOT point at this feature f as contributing. Does SHAP support this kind of explanation?\r\n\r\nIf so, How would it be read from a SHAP plot? Is it enough having the features scaled in [-1,1] for it to be handled naturally? \r\nIf not, how could SHAP be extended to support such statements?\r\n\r\n2. Can SHAP explain an uncertainty based prediction model? I.e.,:\r\n\r\n[2.1] Is SHAP able to explain uncertain (gradual) assignments to classes? For example, our model assigns 0.3 chance that an instance is of class A and 0.7 that it is class B. How it is ‘read’ from the SHAP plots? \r\n\r\n[2.2] Assuming we have those uncertain assignments to classes, is SHAP able to explore them and assess that e.g., \"The model is not certain enough to predict either class A nor class B”?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2344/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2344/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
