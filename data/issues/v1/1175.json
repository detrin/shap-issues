{
  "url": "https://api.github.com/repos/shap/shap/issues/1175",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1175/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1175/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1175/events",
  "html_url": "https://github.com/shap/shap/issues/1175",
  "id": 606061681,
  "node_id": "MDU6SXNzdWU2MDYwNjE2ODE=",
  "number": 1175,
  "title": "custom_record_gradient() error",
  "user": {
    "login": "SaravananOffl",
    "id": 30234192,
    "node_id": "MDQ6VXNlcjMwMjM0MTky",
    "avatar_url": "https://avatars.githubusercontent.com/u/30234192?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/SaravananOffl",
    "html_url": "https://github.com/SaravananOffl",
    "followers_url": "https://api.github.com/users/SaravananOffl/followers",
    "following_url": "https://api.github.com/users/SaravananOffl/following{/other_user}",
    "gists_url": "https://api.github.com/users/SaravananOffl/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/SaravananOffl/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/SaravananOffl/subscriptions",
    "organizations_url": "https://api.github.com/users/SaravananOffl/orgs",
    "repos_url": "https://api.github.com/users/SaravananOffl/repos",
    "events_url": "https://api.github.com/users/SaravananOffl/events{/privacy}",
    "received_events_url": "https://api.github.com/users/SaravananOffl/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2020-04-24T05:56:42Z",
  "updated_at": "2020-04-29T08:40:57Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I'm trying to use shap for a lstm model with the below architecture: \r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nmasking (Masking)            (None, 4, 35)             0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 4, 200)            188800    \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 200)               320800    \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 200)               800       \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 200)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 250)               50250     \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 250)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 50)                12550     \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 50)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 51        \r\n=================================================================\r\nTotal params: 573,251\r\nTrainable params: 572,851\r\nNon-trainable params: 400\r\n_________________________________________________________________\r\n```\r\nTF Version - tensorflow-2.0.0rc1\r\n\r\nwhen I run this snippet on the model, a custom gradient error is raised.\r\n```\r\nexplainer = shap.DeepExplainer(model, X_train[:100])\r\nshap_values = explainer.shap_values(X_test[:10])\r\n```\r\nError: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\n_FallbackException                        Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in transpose(x, perm, name)\r\n  11438         _ctx._context_handle, _ctx._thread_local_data.device_name,\r\n> 11439         \"Transpose\", name, _ctx._post_execution_callbacks, x, perm)\r\n  11440       return _result\r\n\r\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n17 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in transpose_eager_fallback(x, perm, name, ctx)\r\n  11482                              attrs=_attrs, ctx=_ctx, name=name)\r\n  11483   _execute.record_gradient(\r\n> 11484       \"Transpose\", _inputs_flat, _attrs, _result, name)\r\n  11485   _result, = _result\r\n  11486   return _result\r\n\r\nTypeError: custom_record_gradient() takes 4 positional arguments but 5 were given\r\n```\r\n\r\nAny suggestions to solve this issue? ",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1175/reactions",
    "total_count": 2,
    "+1": 2,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1175/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
