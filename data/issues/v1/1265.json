{
  "url": "https://api.github.com/repos/shap/shap/issues/1265",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/1265/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/1265/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/1265/events",
  "html_url": "https://github.com/shap/shap/issues/1265",
  "id": 637398046,
  "node_id": "MDU6SXNzdWU2MzczOTgwNDY=",
  "number": 1265,
  "title": "Additivity for xgboost multiclass model with background data",
  "user": {
    "login": "flian2",
    "id": 15711212,
    "node_id": "MDQ6VXNlcjE1NzExMjEy",
    "avatar_url": "https://avatars.githubusercontent.com/u/15711212?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/flian2",
    "html_url": "https://github.com/flian2",
    "followers_url": "https://api.github.com/users/flian2/followers",
    "following_url": "https://api.github.com/users/flian2/following{/other_user}",
    "gists_url": "https://api.github.com/users/flian2/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/flian2/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/flian2/subscriptions",
    "organizations_url": "https://api.github.com/users/flian2/orgs",
    "repos_url": "https://api.github.com/users/flian2/repos",
    "events_url": "https://api.github.com/users/flian2/events{/privacy}",
    "received_events_url": "https://api.github.com/users/flian2/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2020-06-12T00:08:03Z",
  "updated_at": "2020-06-12T00:08:03Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi Slundberg,\r\n\r\nThanks for the great features in the shap pacakge! \r\n\r\nI have a question regarding whether the additivity should hold for xgboost multiclass models when the background distribution data is supplied. When computing SHAP values on xgboost multiclass models with background data, the SHAP values and the expected value in each class cannot sum up to the margin output of the model itself, though it passed the internal additivity check. Here's an example using the iris dataset. I'm using xgboost-0.9.0, shap-0.35.0.\r\n\r\n\r\n```\r\nfrom sklearn.datasets import load_iris\r\nfrom xgboost import XGBClassifier, DMatrix\r\nfrom shap import TreeExplainer\r\n\r\niris = load_iris()\r\nX = iris.data  # we only take the first two features.\r\ny = iris.target\r\n\r\nmodel = XGBClassifier(max_depth=4, learning_rate=0.1)\r\nmodel.fit(X, y)\r\n# No background distribution\r\nexplainer_tree_path_dependent = TreeExplainer(model=model)\r\nX_input = X[:2, :]\r\nshap_values_1 = explainer_tree_path_dependent.shap_values(X_input)\r\nprint('expected value', explainer_tree_path_dependent.expected_value)\r\n\r\ny_mean_train = model.get_booster().predict(DMatrix(X, feature_names=model.get_booster().feature_names), output_margin=True)\\\r\n    .mean(axis=0)\r\nprint('y_mean_train: ', y_mean_train)\r\n\r\ny_out_margin = model.get_booster().predict(DMatrix(X_input, feature_names=model.get_booster().feature_names), output_margin=True)\r\nfor i in range(0, y_out_margin.shape[1]):\r\n    # this assertion pass\r\n    np.testing.assert_array_almost_equal(explainer_tree_path_dependent.expected_value[i] + np.sum(shap_values_1[i], axis=1),\r\n                                         y_out_margin[:, i], decimal=5)\r\n# Provide background distribution\r\nX_background = X[np.random.choice(range(X.shape[0]), 100)]\r\ny_mean_background = model.get_booster().predict(DMatrix(X_background, feature_names=model.get_booster().feature_names),\r\n                                           output_margin=True) \\\r\n    .mean(axis=0)\r\nprint('y_mean_background: ', y_mean_background)\r\nexplainer_interventional = TreeExplainer(model=model, data=X_background,\r\n                                         feature_perturbation='interventional')\r\nshap_values_2 = explainer_interventional.shap_values(X_input)\r\nprint('expected value: ', explainer_interventional.expected_value)\r\nfor i in range(0, y_out_margin.shape[1]):\r\n    # this assertion fails\r\n    np.testing.assert_array_almost_equal(explainer_interventional.expected_value[i] + np.sum(shap_values_2[i], axis=1),\r\n                                         y_out_margin[:, i], decimal=5)\r\n\r\n```\r\nThe first assersion pass (multi-class model with no background data) while the second (multi-class model with background data) fails. The result I got is\r\n\r\n```\r\nE           AssertionError: \r\nE           Arrays are not almost equal to 5 decimals\r\nE           \r\nE           Mismatch: 100%\r\nE           Max absolute difference: 4.11635102\r\nE           Max relative difference: 1.20383915\r\nE            x: array([-0.697, -0.697])\r\nE            y: array([3.41935, 3.41935], dtype=float32)\r\n```\r\n\r\nAlso the expected value of the tree explainer with background data is not equal to the mean margin output on the background data:\r\n\r\n```\r\nexpected value [0.19756347, 0.54107183, 0.5989219]\r\ny_mean_train:  [-0.42676818 -0.12458882 -0.31768182]\r\ny_mean_background:  [-0.15994754 -0.00579189 -0.72406685]\r\nexpected value:  [-0.73281003 -0.7942796  -0.36271671]\r\n```\r\n\r\nFrom my understanding, the bias term should be be the mean of predictions on the background data if it is supplied, and from previous experiments I found this to hold for binary models.\r\n\r\nAnother thing I note is that the output for ``explainer_interventional.model.predict()`` and ``explainer_interventional.original_model.predict()`` does not have the same result. \r\n\r\n```\r\nprint(explainer_interventional.model.predict(X_input))\r\n>>> [[-0.69699803 -0.84870495 -0.80412233]\r\n [-0.69699803 -0.58326706 -0.7187173 ]]\r\n \r\nprint(explainer_interventional.model.original_model.predict(DMatrix(X_input), output_margin=True, validate_features=False))\r\n>>> [[ 3.419353  -2.0315516 -2.7376275]\r\n [ 3.419353  -2.0315516 -2.3867838]]\r\n ```\r\n\r\nI wonder what is the output for self.model.predict(), if it does not represent the margin output for each class?\r\n\r\nThank you!",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/1265/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/1265/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
