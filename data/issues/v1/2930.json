{
  "url": "https://api.github.com/repos/shap/shap/issues/2930",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2930/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2930/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2930/events",
  "html_url": "https://github.com/shap/shap/issues/2930",
  "id": 1719193201,
  "node_id": "I_kwDOBHDcK85meM5x",
  "number": 2930,
  "title": "IndexError: index out of range in self while using GPT2LMHeadModel.from_pretrained(\"gpt2\")",
  "user": {
    "login": "ffuad",
    "id": 50210878,
    "node_id": "MDQ6VXNlcjUwMjEwODc4",
    "avatar_url": "https://avatars.githubusercontent.com/u/50210878?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ffuad",
    "html_url": "https://github.com/ffuad",
    "followers_url": "https://api.github.com/users/ffuad/followers",
    "following_url": "https://api.github.com/users/ffuad/following{/other_user}",
    "gists_url": "https://api.github.com/users/ffuad/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ffuad/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ffuad/subscriptions",
    "organizations_url": "https://api.github.com/users/ffuad/orgs",
    "repos_url": "https://api.github.com/users/ffuad/repos",
    "events_url": "https://api.github.com/users/ffuad/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ffuad/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2023-05-22T08:40:45Z",
  "updated_at": "2023-05-22T08:40:45Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I am working on this question answering code and using pretrained GPT2LMHeadModel. But after tokenization when I pass the inputs and attention mask to the model it is giving index error. My code:\r\n\r\n```\r\nfeedback_dataset = []\r\n\r\n# Preprocessing\r\nnltk.download(\"stopwords\")\r\nnltk.download(\"wordnet\")\r\n\r\nlemmatizer = WordNetLemmatizer()\r\nstop_words = set(stopwords.words(\"english\"))\r\n\r\ndef preprocess_text(text):\r\n    # Lowercase\r\n    text = text.lower()\r\n    \r\n    # Remove punctuation\r\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\r\n    \r\n    # Remove numbers\r\n    text = re.sub(r\"\\d+\", \"\", text)\r\n    \r\n    # Tokenization\r\n    tokens = text.split()\r\n    \r\n    # Remove stop words\r\n    tokens = [token for token in tokens if token not in stop_words]\r\n    \r\n    # Lemmatization\r\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\r\n    \r\n    # Join tokens\r\n    text = \" \".join(tokens)\r\n    \r\n    return text\r\n\r\n# Preprocess the dataset\r\npreprocessed_dataset = [\r\n    {\r\n        \"user\": preprocess_text(entry[\"user\"]),\r\n        \"bot\": preprocess_text(entry[\"bot\"])\r\n    }\r\n    for entry in dataset\r\n]\r\n\r\n# Load pre-trained model and tokenizer\r\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\r\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\r\n# Add padding token to the tokenizer\r\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\r\n\r\n# Define the maximum sequence length\r\nmax_length = 512  # Set your desired maximum length here\r\n\r\n# Tokenize and format the dataset with truncation\r\ntokenized_dataset = tokenizer.batch_encode_plus(\r\n    [(entry[\"user\"], entry[\"bot\"]) for entry in preprocessed_dataset],\r\n    padding=\"longest\",\r\n    truncation=True,\r\n    max_length=max_length,\r\n    return_tensors=\"pt\"\r\n)\r\n\r\ninput_ids = tokenized_dataset[\"input_ids\"]\r\nattention_mask = tokenized_dataset[\"attention_mask\"]\r\n\r\n# Ensure input tensors have correct shape\r\ninput_ids = input_ids.squeeze()\r\nattention_mask = attention_mask.squeeze()\r\n# Define optimizer and loss function\r\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\r\nloss_fn = torch.nn.CrossEntropyLoss()\r\n\r\n# Training loop\r\nnum_epochs = 2\r\nfor epoch in range(num_epochs):\r\n    optimizer.zero_grad()\r\n    inputs = {\r\n        \"input_ids\": input_ids,\r\n        \"attention_mask\": attention_mask,\r\n        \"labels\": input_ids\r\n    }\r\n    print(\"input_ids shape: \", input_ids.shape,\"attention_mask shape: \", attention_mask.shape)#, \"input shape: \", inputs)\r\n    \r\n    outputs = model(**inputs)\r\n    loss = outputs.loss\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n```\r\n\r\nI am getting error in the ```outpus =  model(**inputs)```  line. The error is:\r\n\r\n```\r\ninput_ids shape:  torch.Size([5, 19]) attention_mask shape:  torch.Size([5, 19])\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-39-3329f43b161a> in <cell line: 7>()\r\n     14     print(\"input_ids shape: \", input_ids.shape,\"attention_mask shape: \", attention_mask.shape)#, \"input shape: \", inputs)\r\n     15 \r\n---> 16     outputs = model(**inputs)\r\n     17     loss = outputs.loss\r\n     18     loss.backward()\r\n\r\n6 frames\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args, **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\r\n   1074         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n   1075 \r\n-> 1076         transformer_outputs = self.transformer(\r\n   1077             input_ids,\r\n   1078             past_key_values=past_key_values,\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args, **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\r\n    841 \r\n    842         if inputs_embeds is None:\r\n--> 843             inputs_embeds = self.wte(input_ids)\r\n    844         position_embeds = self.wpe(position_ids)\r\n    845         hidden_states = inputs_embeds + position_embeds\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args, **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py in forward(self, input)\r\n    160 \r\n    161     def forward(self, input: Tensor) -> Tensor:\r\n--> 162         return F.embedding(\r\n    163             input, self.weight, self.padding_idx, self.max_norm,\r\n    164             self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\r\n   2208         # remove once script supports set_grad_enabled\r\n   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n   2211 \r\n   2212 \r\n\r\nIndexError: index out of range in self\r\n```\r\nthe size of input and attention mask is same. And it also the shape of token is also less than 1024 which is max for gpt2. So what could be the problem? Can anyone help me please.\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2930/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2930/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
