{
  "url": "https://api.github.com/repos/shap/shap/issues/2872",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2872/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2872/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2872/events",
  "html_url": "https://github.com/shap/shap/issues/2872",
  "id": 1612643983,
  "node_id": "I_kwDOBHDcK85gHv6P",
  "number": 2872,
  "title": "How does shap work when the network has two different dimensions of input? Can anyone provide relevant experience?",
  "user": {
    "login": "NadirZhang",
    "id": 113686369,
    "node_id": "U_kgDOBsa3YQ",
    "avatar_url": "https://avatars.githubusercontent.com/u/113686369?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/NadirZhang",
    "html_url": "https://github.com/NadirZhang",
    "followers_url": "https://api.github.com/users/NadirZhang/followers",
    "following_url": "https://api.github.com/users/NadirZhang/following{/other_user}",
    "gists_url": "https://api.github.com/users/NadirZhang/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/NadirZhang/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/NadirZhang/subscriptions",
    "organizations_url": "https://api.github.com/users/NadirZhang/orgs",
    "repos_url": "https://api.github.com/users/NadirZhang/repos",
    "events_url": "https://api.github.com/users/NadirZhang/events{/privacy}",
    "received_events_url": "https://api.github.com/users/NadirZhang/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "closed",
  "locked": true,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2023-03-07T03:22:17Z",
  "updated_at": "2023-06-03T23:33:29Z",
  "closed_at": "2023-06-03T23:33:29Z",
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "How does shap work when the network has two different dimensions of input? Can anyone provide relevant experience?\r\n\r\n\r\n# input\r\nX =  [batch_size, 116, 116]\r\nZ =  [batch_size, 116, 3]\r\n\r\nErrorï¼šThe size of tensor a (116) must match the size of tensor b (3) at non-singleton dimension 2\r\n\r\n\r\n\r\n\r\n# Detail\r\n```\r\nTraceback (most recent call last):\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/PycharmProjects/Ensamble3dCNN/NEGAN/k-fold-shap.py\", line 185, in <module>\r\n    shap_values = explainer.shap_values(test_data)\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/shap/explainers/_deep/__init__.py\", line 124, in shap_values\r\n    return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/shap/explainers/_deep/deep_pytorch.py\", line 185, in shap_values\r\n    sample_phis = self.gradient(feature_ind, joint_x)\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/shap/explainers/_deep/deep_pytorch.py\", line 121, in gradient\r\n    grad = torch.autograd.grad(selected, x,\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 234, in grad\r\n    return Variable._execution_engine.run_backward(\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/shap/explainers/_deep/deep_pytorch.py\", line 226, in deeplift_grad\r\n    return op_handler[module_type](module, grad_input, grad_output)\r\n  File \"/remote-home/CS_IMIPAD_public2/zhangzhuan22/.conda/envs/pt110/lib/python3.8/site-packages/shap/explainers/_deep/deep_pytorch.py\", line 358, in nonlinear_1d\r\n    grad_output[0] * (delta_out / delta_in).repeat(dup0))\r\nRuntimeError: The size of tensor a (116) must match the size of tensor b (3) at non-singleton dimension 2\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n\r\n# Code\r\n```\r\nclass NEGAN_fusion(nn.Module):\r\n    def __init__(self, layer):\r\n        super(NEGAN_fusion, self).__init__()\r\n        self.layer = layer\r\n        self.relu = nn.ReLU()\r\n        self.atten = nn.ModuleList([Attention() for i in range(layer)])\r\n        self.norm_n = nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\r\n        self.norm_e = nn.ModuleList([nn.BatchNorm1d(116) for i in range(layer)])\r\n        self.node_w = nn.ParameterList([nn.Parameter(torch.randn((3, 3), dtype=torch.float32)) for i in range(layer)])\r\n        self.edge_w = nn.ParameterList(\r\n            [nn.Parameter(torch.randn((116, 116), dtype=torch.float32)) for i in range(layer)])\r\n        self.line_n = nn.ModuleList(\r\n            [nn.Sequential(nn.Linear(116 * 3, 128), nn.ReLU(), nn.BatchNorm1d(128)) for i in range(layer + 1)])\r\n        self.line_e = nn.ModuleList(\r\n            [nn.Sequential(nn.Linear(116 * 116, 128 * 3), nn.ReLU(), nn.BatchNorm1d(128 * 3)) for i in\r\n             range(layer + 1)])\r\n        self.clase = nn.Sequential(nn.Linear(128 * 4 * (self.layer + 1), 1024), nn.Dropout(0.2), nn.ReLU(),\r\n                                   nn.Linear(1024, 2))\r\n        self.ones = nn.Parameter(torch.ones((116), dtype=torch.float32), requires_grad=False)\r\n\r\n    def forward(self, X, Z):\r\n        XX = self.line_n[0](X.reshape(X.size()[0], -1))  # flatten\r\n        ZZ = self.line_e[0](Z.reshape(X.size()[0], -1))\r\n        for i in range(self.layer):\r\n            A = self.atten[i](Z, X)\r\n            Z1 = torch.matmul(A, Z)\r\n            Z2 = torch.matmul(Z1, self.edge_w[i])\r\n            Z = self.relu(self.norm_e[i](Z2)) + Z\r\n            ZZ = torch.cat((ZZ, self.line_e[i + 1](Z.reshape(X.size()[0], -1))), dim=1)\r\n            X1 = torch.matmul(A, X)\r\n            X1 = torch.matmul(X1, self.node_w[i])\r\n            X = self.relu(self.norm_n[i](X1)) + X\r\n            XX = torch.cat((XX, self.line_n[i + 1](X.reshape(X.size()[0], -1))), dim=1)\r\n        XZ = torch.cat((XX, ZZ), 1)\r\n        y = self.clase(XZ)\r\n        return y\r\n\r\ndef obtain_data(batch_size=16):\r\n    train_asd = train_asd_dict[0]\r\n    train_tdc = train_tdc_dict[0]\r\n    test_asd = test_asd_dict[0]\r\n    test_tdc = test_tdc_dict[0]\r\n    trainset = dataset(site=train_site, fmri_root=cpac_root, smri_root=smri_root, ASD=train_asd, TDC=train_tdc)\r\n    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=False)\r\n    testset = dataset(site=test_site, fmri_root=cpac_root, smri_root=smri_root, ASD=test_asd, TDC=test_tdc)\r\n    testloader = DataLoader(testset, batch_size=batch_size)\r\n    return trainloader, testloader\r\n\r\ndef NEGAN_proc_input(loader):\r\n    temp_X, temp_Z, temp_label, temp_sub_id = next(iter(loader))\r\n    # print(temp_X.shape, temp_Z.shape)\r\n    output = [temp_X, temp_Z]\r\n    return temp_X, temp_Z, output\r\n\r\npath = '/remote-home/CS_IMIPAD_public2/zhangzhuan22/PycharmProjects/NEGAN/preproc/saveModels/normtrained/models_0_9'\r\ntrainloader, testloader = obtain_data()\r\n\r\nX_train, Z_train, train_data = NEGAN_proc_input(trainloader)\r\nX_test, Z_test, test_data = NEGAN_proc_input(testloader)\r\n\r\nmodel = NEGAN_fusion(5)\r\nmodel.load_state_dict(torch.load(path))\r\nmodel.eval()\r\n\r\nexplainer = shap.DeepExplainer(model, train_data)\r\nshap_values = explainer.shap_values(test_data)\r\n```\r\n\r\n# Env\r\npytorch 1.10.0\r\nshap 0.41.0",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2872/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2872/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
