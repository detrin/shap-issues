{
  "url": "https://api.github.com/repos/shap/shap/issues/2874",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2874/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2874/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2874/events",
  "html_url": "https://github.com/shap/shap/issues/2874",
  "id": 1618394173,
  "node_id": "I_kwDOBHDcK85gdrw9",
  "number": 2874,
  "title": "Any support for Transformer with tabular timeseries 3D inputs?",
  "user": {
    "login": "greenlight2000",
    "id": 54571788,
    "node_id": "MDQ6VXNlcjU0NTcxNzg4",
    "avatar_url": "https://avatars.githubusercontent.com/u/54571788?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/greenlight2000",
    "html_url": "https://github.com/greenlight2000",
    "followers_url": "https://api.github.com/users/greenlight2000/followers",
    "following_url": "https://api.github.com/users/greenlight2000/following{/other_user}",
    "gists_url": "https://api.github.com/users/greenlight2000/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/greenlight2000/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/greenlight2000/subscriptions",
    "organizations_url": "https://api.github.com/users/greenlight2000/orgs",
    "repos_url": "https://api.github.com/users/greenlight2000/repos",
    "events_url": "https://api.github.com/users/greenlight2000/events{/privacy}",
    "received_events_url": "https://api.github.com/users/greenlight2000/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2023-03-10T05:36:50Z",
  "updated_at": "2023-03-10T05:36:50Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "I used `deepExplainer` for explaining a pytorch-implemented classification model(basically multiple layers of transformer encoders succeeded by a classifier). My data is tabular timeseries in the shape of (sample, time_step, features). Note that the time sequence samples have various lengths and the model pads them into same length, and mask out the padded part in the attention mechanism.\r\nMy code runs well(though very very slow), but the calculated shapley values sums up to a wrong value:\r\n```\r\nexplainer = shap.DeepExplainer(model, X)\r\nshap_values = explainer.shap_values(X)\r\n\r\nsample_no = 1 # test the calculated shapley value validity on a random sample # x\r\nclass = 0# say binary classification and there is a two-dimension output\r\npred_val = predp_for_sample = model(X_test)[sample_no][pred] # f(x)\r\nshapley_sum_on_timestepNfeatures = shap_values[pred][sample_no].sum() #\\sum of phi_i\r\nexpected_val = expected_value[pred] # phi_0\r\n```\r\n it turns out `shapley_sum_on_timestepNfeatures + expected_val` doesn't approximate to `pred_val`, which means the calculated shapley value on transformer is invalid.\r\n\r\nI then looked into the shapley value, I found that the padded timesteps are also assigned shapeley values(supposed to be 0). It suggest that the my way of calling SHAP API is not capable of dealing with the `masking` mechanism in  Transformer. \r\n- is there any way I'm doing it wrong? any correct api usage that I can use to do my task?\r\n- if SHAP is currently not supporting transformer with various-length tabular timeseries inputs, any other efficient explainable models/algorithms I can look into?\r\n- If I want to modified SHAP for my own use, any support document or tutorial I can look into?",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2874/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2874/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
