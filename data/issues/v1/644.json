{
  "url": "https://api.github.com/repos/shap/shap/issues/644",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/644/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/644/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/644/events",
  "html_url": "https://github.com/shap/shap/issues/644",
  "id": 455613591,
  "node_id": "MDU6SXNzdWU0NTU2MTM1OTE=",
  "number": 644,
  "title": "Explaining stateful LSTM model",
  "user": {
    "login": "YMaks",
    "id": 22409396,
    "node_id": "MDQ6VXNlcjIyNDA5Mzk2",
    "avatar_url": "https://avatars.githubusercontent.com/u/22409396?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/YMaks",
    "html_url": "https://github.com/YMaks",
    "followers_url": "https://api.github.com/users/YMaks/followers",
    "following_url": "https://api.github.com/users/YMaks/following{/other_user}",
    "gists_url": "https://api.github.com/users/YMaks/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/YMaks/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/YMaks/subscriptions",
    "organizations_url": "https://api.github.com/users/YMaks/orgs",
    "repos_url": "https://api.github.com/users/YMaks/repos",
    "events_url": "https://api.github.com/users/YMaks/events{/privacy}",
    "received_events_url": "https://api.github.com/users/YMaks/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 5,
  "created_at": "2019-06-13T08:49:36Z",
  "updated_at": "2019-09-30T14:26:41Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hello. I'm trying to use DeepExplainer and KernelExplainer to explain  stateful LSTM model (keras implementation) with batch_size = 1 and batch_shape = (1, 60).\r\nTested attempts:\r\n\r\n1. DeepExplainer\r\n`melting_train = melting_train[0].reshape(1, 60)`\r\n `explainer = shap.DeepExplainer(model, melting_train)`\r\n`melting_test = melting_test[0].reshape(1, 60)`\r\n`shap_values = explainer.shap_values(melting_test)`\r\n\r\nGot `ValueError: Dimension size must be evenly divisible by 2 but is 1\r\nNumber of ways to split should evenly divide the split dimension for 'gradients/dense_2/Sigmoid_grad/split' (op: 'Split') with input shapes: [], [1,1] and with computed input tensors: input[0] = <0>`\r\n\r\n2. KernelExplainer\r\n`melting_train = melting_train[0].reshape(1, 60)`\r\n`explainer = shap.KernelExplainer(model.predict_on_batch, melting_train)`\r\n`melting_test = melting_test[0].reshape(1, 60)`\r\n`shap_values = explainer.shap_values(melting_test, nsamples=1)`\r\n\r\nPassed, but the result of plotting was empty plots.\r\n`shap.force_plot(explainer.expected_value[0], shap_values[0][0], melting_test[0], feature_names=feature_names)`\r\n`shap.summary_plot(shap_values, melting_test[0], plot_type='bar', feature_names=feature_names)`\r\n\r\nI guess the reason is passing only one train example to explainer but according to the model architecture it cannot process more than one batch per run.\r\n\r\nSo, is there any way to create explainer and pass train data batch by batch or is there any other way to use shap for explaining stateful LSTM models?\r\n\r\nThanks.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/644/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/644/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
