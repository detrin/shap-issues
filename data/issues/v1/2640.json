{
  "url": "https://api.github.com/repos/shap/shap/issues/2640",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2640/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2640/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2640/events",
  "html_url": "https://github.com/shap/shap/issues/2640",
  "id": 1326416580,
  "node_id": "I_kwDOBHDcK85PD4LE",
  "number": 2640,
  "title": "Why would shap beeswarm plot look different depending on model output?",
  "user": {
    "login": "kevkid",
    "id": 1369930,
    "node_id": "MDQ6VXNlcjEzNjk5MzA=",
    "avatar_url": "https://avatars.githubusercontent.com/u/1369930?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/kevkid",
    "html_url": "https://github.com/kevkid",
    "followers_url": "https://api.github.com/users/kevkid/followers",
    "following_url": "https://api.github.com/users/kevkid/following{/other_user}",
    "gists_url": "https://api.github.com/users/kevkid/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/kevkid/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/kevkid/subscriptions",
    "organizations_url": "https://api.github.com/users/kevkid/orgs",
    "repos_url": "https://api.github.com/users/kevkid/repos",
    "events_url": "https://api.github.com/users/kevkid/events{/privacy}",
    "received_events_url": "https://api.github.com/users/kevkid/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2022-08-02T21:26:54Z",
  "updated_at": "2022-08-02T21:26:54Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hi I have a dataset that I have trained an xgboost model via gridsearch CV. I was able to run the treeexplainer on it but when I use model_output = 'probability' I get a shap plot that looks awfully sparse:\r\nOmitted var names:\r\n![image](https://user-images.githubusercontent.com/1369930/182475099-d96de29c-7e42-4364-b83b-f1687bf7439d.png)\r\n\r\nbut when I run the same exact model using output raw I get:\r\n![image](https://user-images.githubusercontent.com/1369930/182475823-116f654a-3773-4ac3-a27b-52cd3fe99995.png)\r\n\r\n\r\nIf I were to sum up the shap values for a row (+ expected value) I get the probability of the model which is indeed correct. What I dont understand is why the figures look so different?\r\n\r\nFurthermore I explored this concept using dummy data:\r\n```\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nimport xgboost as xgb\r\ntoy_data_X, toy_data_Y = make_classification(500, 20,)\r\nX_train_toy_data, X_test_toy_data, y_train_toy_data, y_test_toy_data = train_test_split(toy_data_X, toy_data_Y, test_size=0.2)\r\nclassify_xgb = xgb.XGBClassifier(\r\n    objective = 'binary:logistic',\r\n    #missing = nan,\r\n    seed = SEED,\r\n    scale_pos_weight = 400, # approx \r\n    n_estimators=200\r\n)\r\nfrom sklearn.model_selection import GridSearchCV\r\nparameters = {#'nthread':[2], #when use hyperthread, xgboost may become slower\r\n              'objective':['binary:logistic'],\r\n              'learning_rate': [0.3, 0.4, 0.5], #so called `eta` value\r\n              'max_depth': [x for x in range(4, 10)],\r\n            }\r\nclassify_xgb_GS = GridSearchCV(classify_xgb, parameters, n_jobs=1, \r\n                                scoring='roc_auc',#make_scorer(f1_score, average='binary'),#'roc_auc',\r\n                                cv=5,\r\n                                verbose=2, refit=True)\r\nclassify_xgb_GS.fit(X_train_toy_data, y_train_toy_data)\r\nexplainer = shap.TreeExplainer(classify_xgb_GS.best_estimator_, toy_data_X, feature_perturbation='interventional', model_output=\"probability\")\r\nshap_values = explainer.shap_values(toy_data_X)\r\nshap_obj = explainer(toy_data_X)\r\n# probability\r\ndc_shap_obj = deepcopy(shap_obj)\r\n#dc_shap_obj.feature_names = [feature_name_dict[x] if x in feature_name_dict else x for x in dc_shap_obj.feature_names]\r\nshap.plots.beeswarm(dc_shap_obj, max_display=10, plot_size=1)\r\n```\r\nand I get this for model_output = probability:\r\n![image](https://user-images.githubusercontent.com/1369930/182476089-f3da3b74-da90-4757-b52d-8479c99d7ad3.png)\r\n\r\nand for model_output = raw:\r\n![image](https://user-images.githubusercontent.com/1369930/182476139-8fd782b7-a323-4284-8f1c-95a716eb8cfb.png)\r\n\r\nNotice how they look \"similar\" in the sense that there is a spread and looks much less sparse. The real dataset has 10k samples while the dummy dataset has 500.\r\n\r\nWhy would something like this happen? ",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2640/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2640/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
