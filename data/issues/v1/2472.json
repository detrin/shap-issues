{
  "url": "https://api.github.com/repos/shap/shap/issues/2472",
  "repository_url": "https://api.github.com/repos/shap/shap",
  "labels_url": "https://api.github.com/repos/shap/shap/issues/2472/labels{/name}",
  "comments_url": "https://api.github.com/repos/shap/shap/issues/2472/comments",
  "events_url": "https://api.github.com/repos/shap/shap/issues/2472/events",
  "html_url": "https://github.com/shap/shap/issues/2472",
  "id": 1189799584,
  "node_id": "I_kwDOBHDcK85G6uag",
  "number": 2472,
  "title": "Question - Deep and Gradient Explainers with PyTorch and Embeddings",
  "user": {
    "login": "pdurham2",
    "id": 17155167,
    "node_id": "MDQ6VXNlcjE3MTU1MTY3",
    "avatar_url": "https://avatars.githubusercontent.com/u/17155167?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/pdurham2",
    "html_url": "https://github.com/pdurham2",
    "followers_url": "https://api.github.com/users/pdurham2/followers",
    "following_url": "https://api.github.com/users/pdurham2/following{/other_user}",
    "gists_url": "https://api.github.com/users/pdurham2/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/pdurham2/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/pdurham2/subscriptions",
    "organizations_url": "https://api.github.com/users/pdurham2/orgs",
    "repos_url": "https://api.github.com/users/pdurham2/repos",
    "events_url": "https://api.github.com/users/pdurham2/events{/privacy}",
    "received_events_url": "https://api.github.com/users/pdurham2/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2022-04-01T13:29:24Z",
  "updated_at": "2022-06-03T09:27:48Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "Hello, I am trying to use shap with a PyTorch model that uses embeddings and I was wondering if there are any suggestions/examples for how to accomplish this. I found a couple issues from 2019/2020 (#530 and #1039) that address the limitation of pytorch embeddings since they require longs and gradients are only supported with floats, but have there been any developments on this front?\r\n\r\nFollowing the lead in #1039, I also looked at using the Kernel or Permutation explainers but the model I am using does not really fit a tabular dataset since it is using an encoder/decoder structure with different sequence lengths.\r\n\r\nThank you!\r\n\r\n\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/shap/shap/issues/2472/reactions",
    "total_count": 4,
    "+1": 4,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/shap/shap/issues/2472/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
